[ { "title": "Extending MediatR with publishing strategies", "url": "/posts/mediatr-publishing-strategies/", "categories": "Tutorial, Software Development", "tags": "Mediator", "date": "2024-05-30 13:00:00 +0200", "snippet": " Update: I published a NuGet package that extends MediatR with these features. You can find it here.A few years ago I published an article on how to utilize different publishing strategies for Medi...", "content": " Update: I published a NuGet package that extends MediatR with these features. You can find it here.A few years ago I published an article on how to utilize different publishing strategies for MediatR notifications. However, the authors introduced breaking changes in the API and the internals since then, so the approaches described in that article won’t work for versions 12 and onward.Starting with version 12, we can provide a custom INotificationPublisher implementation during the registration.services.AddMediatR(cfg =&gt; { cfg.RegisterServicesFromAssemblyContaining&lt;Program&gt;(); cfg.NotificationPublisher = new MyCustomPublisher(); // this will be singleton cfg.NotificationPublisherType = typeof(MyCustomPublisher); // this will be the ServiceLifetime});This new capability offers some flexibility, but it’s still not exactly what we want. Depending on various circumstances, we may want to publish different notifications using different strategies or even different strategies for the same notification. So, let’s define our requirements. Implement multiple publishing strategies We should be able to choose a strategy while publishing a notification. Ideally, the feature should be an extension to IPublisher. Consumers should not have to deal with new types.Publishing StrategiesLet’s first create an enum with the strategies we plan on supporting.public enum PublishStrategy{ /// &lt;summary&gt; /// The default publisher or the one set in MediatR configuration. /// &lt;/summary&gt; Default = 0, /// &lt;summary&gt; /// Executes and awaits each notification handler after one another. /// Returns when all handlers complete or an exception has been thrown. /// In case of an exception, the rest of the handlers are not executed. /// &lt;/summary&gt; Sequential = 1, /// &lt;summary&gt; /// Executes and awaits each notification handler after one another. /// Returns when all handlers complete. It continues on exception(s). /// In case of any exception(s), they will be captured in an AggregateException. /// &lt;/summary&gt; SequentialAll = 2,}Extending MediatorNext, let’s define the INotificationPublisher implementations. You may notice that SequentialPublisher is the same as the built-in one. I picked them just as an example to demonstrate how to hook them up. You’ll define your desired custom publishers.public class SequentialPublisher : INotificationPublisher{ public async Task Publish( IEnumerable&lt;NotificationHandlerExecutor&gt; handlerExecutors, INotification notification, CancellationToken cancellationToken) { foreach (var handler in handlerExecutors) { await handler.HandlerCallback(notification, cancellationToken).ConfigureAwait(false); } }}public class SequentialAllPublisher : INotificationPublisher{ public async Task Publish( IEnumerable&lt;NotificationHandlerExecutor&gt; handlerExecutors, INotification notification, CancellationToken cancellationToken) { List&lt;Exception&gt;? exceptions = null; foreach (var handlerExecutor in handlerExecutors) { try { await handlerExecutor.HandlerCallback(notification, cancellationToken).ConfigureAwait(false); } catch (AggregateException ex) { (exceptions ??= []).AddRange(ex.Flatten().InnerExceptions); } catch (Exception ex) { (exceptions ??= []).Add(ex); } } if (exceptions?.Count &gt; 0) { throw new AggregateException(exceptions); } }}The Mediator implementation expects this publisher implementation to be provided in the constructor, along with the IServiceProvider. So, we can not just do this, and call it a day.public static Task Publish&lt;TNotification&gt;( this IPublisher publisher, TNotification notification, PublishStrategy strategy, CancellationToken cancellationToken = default) where TNotification : INotification{ INotificationPublisher? notificationPublisher = strategy switch { PublishStrategy.Sequential =&gt; new SequentialPublisher(), PublishStrategy.SequentialAll =&gt; new SequentialAllPublisher(), _ =&gt; null }; return notificationPublisher is null ? publisher.Publish(notification, cancellationToken) // We need to provide the IServiceProvider : new Mediator(.., notificationPublisher).Publish(notification, cancellationToken);}Instead, we must create our custom mediator implementation, register it during registration, and define the necessary extensions. The ExtendedMediator inherits Mediator and defines an additional Publish method that accepts the strategy as a parameter. Then, we create a new instance of Mediator and pass the chosen publisher. The Mediator implementation is a very lightweight object (contains only two references as a state), so creating a new instance is an acceptable approach. But, what if the consumer has defined a different lifetime (e.g. scoped) during configuration? That’s not an issue since we’re passing the resolved IServiceProvider. In the case of a scoped lifetime, the service provider itself is scoped and the behavior will be preserved.public class ExtendedMediator(IServiceProvider serviceProvider) : Mediator(serviceProvider){ private readonly IServiceProvider _serviceProvider = serviceProvider; private static readonly Dictionary&lt;PublishStrategy, INotificationPublisher&gt; _publishers = new() { [PublishStrategy.Sequential] = new SequentialPublisher(), [PublishStrategy.SequentialAll] = new SequentialAllPublisher(), }; public Task Publish&lt;TNotification&gt;( TNotification notification, PublishStrategy strategy, CancellationToken cancellationToken = default) where TNotification : INotification { if (_publishers.TryGetValue(strategy, out var publisher)) { new Mediator(_serviceProvider, publisher).Publish(notification, cancellationToken); } return Publish(notification, cancellationToken); }}public static class MediatorExtensions{ public static Task Publish&lt;TNotification&gt;( this IPublisher publisher, TNotification notification, PublishStrategy strategy, CancellationToken cancellationToken = default) where TNotification : INotification { return publisher is ExtendedMediator extendedMediator ? extendedMediator.Publish(notification, strategy, cancellationToken) : throw new NotSupportedException(\"The extended mediator implementation is not registered! Register it with the IServiceCollection.AddExtendedMediatR extensions.\"); } public static IServiceCollection AddExtendedMediatR( this IServiceCollection services, Action&lt;MediatRServiceConfiguration&gt; configuration) { var serviceConfig = new MediatRServiceConfiguration(); configuration.Invoke(serviceConfig); return services.AddExtendedMediatR(serviceConfig); } public static IServiceCollection AddExtendedMediatR( this IServiceCollection services, MediatRServiceConfiguration configuration) { configuration.MediatorImplementationType = typeof(ExtendedMediator); services.AddMediatR(configuration); return services; }}UsageNow that we’re all set, the usage is quite straightforward.builder.Services.AddExtendedMediatR(cfg =&gt;{ // All your desired configuration. cfg.RegisterServicesFromAssemblyContaining&lt;Program&gt;();\t // Our extension will always set the MediatorImplementationType to ExtendedMediator.});public class Foo(IPublisher publisher){ public async Task Run(CancellationToken cancellationToken) { // The built-in behavior await publisher.Publish(new Ping(), cancellationToken); // Publish with specific strategy await publisher.Publish(new Ping(), PublishStrategy.SequentialAll, cancellationToken); }}Background TasksThere might be cases where you want to publish a notification and not wait for the handler’s completion. Simply, you want them to run as background tasks. I’d avoid this approach and use these strategies sparingly, however, you might have a valid use case. To implement this feature, we might be compelled to just add a new publisher as follows.public enum PublishStrategy{ Default = 0, Sequential = 1, SequentialAll = 2, SequentialBackground = 3, SequentialAllBackground = 4,}public class SequentialBackgroundPublisher : INotificationPublisher{ public Task Publish( IEnumerable&lt;NotificationHandlerExecutor&gt; handlerExecutors, INotification notification, CancellationToken cancellationToken) { _ = Task.Run(async () =&gt; { try { foreach (var handler in handlerExecutors) { await handler.HandlerCallback(notification, cancellationToken).ConfigureAwait(false); } } catch (Exception ex) { // Handle as needed. } }, cancellationToken); return Task.CompletedTask; }}However, this is a bad idea. This implementation won’t behave correctly; for not so obvious reasons. If the consumer has defined a scoped lifetime, once the request is completed, the scope and all resolved dependencies out of that scope will be disposed. The background task we’ve defined may outlive the request, and our handlers will end up with disposed dependencies.We must run the background task in a separate scope. Moreover, we can’t create a scope out of the injected IServiceProvider since the provider itself is scoped. Instead, we need a separate scope out of the root provider. That said, the updated implementation would be as follows.internal class ExtendedMediator( IServiceScopeFactory serviceScopeFactory, IServiceProvider serviceProvider) : Mediator(serviceProvider){ private readonly IServiceScopeFactory _serviceScopeFactory = serviceScopeFactory; private readonly IServiceProvider _serviceProvider = serviceProvider; private static readonly Dictionary&lt;PublishStrategy, (INotificationPublisher Publisher, bool IsBackgroundTask)&gt; _publishers = new() { [PublishStrategy.Sequential] = (new SequentialPublisher(), false), [PublishStrategy.SequentialAll] = (new SequentialAllPublisher(), false), [PublishStrategy.SequentialBackground] = (new SequentialPublisher(), true), [PublishStrategy.SequentialAllBackground] = (new SequentialAllPublisher(), true), }; public Task Publish&lt;TNotification&gt;( TNotification notification, PublishStrategy strategy, CancellationToken cancellationToken = default) where TNotification : INotification { if (_publishers.TryGetValue(strategy, out (INotificationPublisher Publisher, bool IsBackgroundTask) item)) { return item.IsBackgroundTask ? PublishBackground(_serviceScopeFactory, notification, item.Publisher, cancellationToken) : Publish(_serviceProvider, notification, item.Publisher, cancellationToken); } // Fall back to the default behavior return Publish(notification, cancellationToken); } private static Task Publish&lt;TNotification&gt;( IServiceProvider serviceProvider, TNotification notification, INotificationPublisher publisher, CancellationToken cancellationToken) where TNotification : INotification =&gt; new Mediator(serviceProvider, publisher).Publish(notification, cancellationToken); private static Task PublishBackground&lt;TNotification&gt;( IServiceScopeFactory serviceScopeFactory, TNotification notification, INotificationPublisher publisher, CancellationToken cancellationToken) where TNotification : INotification { _ = Task.Run(async () =&gt; { using var scope = serviceScopeFactory.CreateScope(); var logger = scope.ServiceProvider.GetService&lt;ILogger&lt;ExtendedMediator&gt;&gt;(); try { var mediator = new Mediator(scope.ServiceProvider, publisher); await mediator.Publish(notification, cancellationToken).ConfigureAwait(false); } catch (Exception ex) { // The aggregate exceptions are already flattened by the publishers. logger?.LogError(ex, \"Error occurred while executing the handler(s) in a background thread!\"); } }, cancellationToken); return Task.CompletedTask; }}This implementation also offers the ability to wrap any publisher in a background task. So, instead of having separate Background enum items, you may define an additional runAsBackgroundTask parameter for the Publish method. That’s up to you to decide which API is more convenient for you.I hope you found this article useful. Happy coding!" }, { "title": "From Hours to Seconds: The Journey to a 630x Faster Batch Job", "url": "/posts/the-journey-to-630x-faster-batch-job/", "categories": "Software Development", "tags": "dotnet", "date": "2024-01-28 12:00:00 +0100", "snippet": "Some time ago, a client needed some help optimizing a batch job for speed. The company provides analytics services for large global organizations in the automotive industry. They receive large amou...", "content": "Some time ago, a client needed some help optimizing a batch job for speed. The company provides analytics services for large global organizations in the automotive industry. They receive large amounts of data from the retailers/dealers and try to make sense of it. The integration is often rudimentary, where most retailers share data in the form of files (e.g. CSV or any other format). Batch jobs are used to process the files and prepare the data for further analysis. More often than not, the data is in bad shape, and batch jobs are not trivial at all. Some of the jobs, during the initial snapshotting, took more than a day to complete. This affected the onboarding processes and it was crucial to reduce the time to some acceptable level.We did manage to improve the process and overall turned out to be a success story. Anyhow, in this article, I want to focus only on a particular task, which I found interesting and worth sharing. The logic in hand took ~42 minutes to complete, and we reduced it to 3-4 seconds. That’s more than 600x improvement in speed. The requirements were as follows. We receive a large file containing Part information. Each record should be curated and processed accordingly. The Part records contain PartNumber information. This data is not always sent in a standardized form and we have to find the correct PartNumber from our MasterPart dataset. We have to find a match according to the following rules: For each Part, find the MasterPart where the Part.PartNumber is at least a suffix to MasterPart.PartNumber. If not found, then try to find the match within MasterPart.PartNumberNoHyphens. Some retailers have removed the hyphens from their part numbers. If not found, apply the opposite search. Find the match where MasterPart.PartNumber is a suffix to Part.PartNumber. Some retailers add additional prefixes to the part numbers. If the length of the Part.PartNumber is less than 3 characters, do not try to find a match.The requirements at first may seem strange, but they have tried many variations and this logic turned out to yield the best results. They know the business domain better, so we won’t try to interfere with these rules but focus on the technical aspects only. There are some assumptions that we can make though. We agreed to the following. It’s safe to assume the PartNumber is less than 50 characters long (usually it is in the range of 10-20 characters). It contains only ASCII characters. It’s ok to trade memory for speed. Most of the time there is no “free lunch”, and if necessary we can make that tradeoff. In this case, the speed is crucial for the business.Implementation 1 (original)The original implementation is shown in the below snippet. I stripped out all the other tasks, the focus of this article and the benchmarks is only on this isolated logic.Part[] parts = []; // Loaded from file or other sources. ~1.5 million records.MasterPart[] masterParts = File // Loaded from file or other sources. ~80K records. .ReadAllLines(\"master-parts.txt\") .Select(x =&gt; new MasterPart(x)) .ToArray();foreach (var part in parts){ // Part contains more properties than just PartNumber // and there is additional processing in this loop. // But, that's not the focus of these benchmarks. var masterPart = FindMatchedPart(part.PartNumber);}MasterPart? FindMatchedPart(string partNumber){ partNumber = partNumber.Trim(); if (partNumber.Length &lt; 3) return null; partNumber = partNumber.ToUpper(); var masterPart = masterParts.FirstOrDefault(x =&gt; x.PartNumber.EndsWith(partNumber)); masterPart ??= masterParts.FirstOrDefault(x =&gt; x.PartNumberNoHyphens.EndsWith(partNumber)); masterPart ??= masterParts.FirstOrDefault(x =&gt; partNumber.EndsWith(x.PartNumber)); return masterPart;}public class Part{ public string PartNumber { get; set; } // Part contains more properties, but they are not relevant to the demo.}public class MasterPart{ public string PartNumber { get; } public string PartNumberNoHyphens { get; } public MasterPart(string partNumber) { PartNumber = partNumber.ToUpper().Trim(); PartNumberNoHyphens = PartNumber.Replace(\"-\", \"\"); }}Considering all the requirements, I’d say the original author did a fair job. For each part, they looped through all MasterParts and tried to find a match. If no match is found, only then do they loop again based on the additional rules. They couldn’t build dictionaries easily, since we’re searching for suffixes and not exact matches. It’s also worth mentioning that EndsWith is highly optimized in .NET. We can eventually argue that using LINQ might hurt the performance. It’s easy to be judgemental, but frankly, in common scenarios and for smaller datasets this would be my first shot too. If nothing else, just to set the baseline for further improvements.However, once this needs to be scaled and applied to large datasets then the issue is evident. We have an O(m*n) operation. There are 80K Part records and 1.5M MasterPart records. So, in the worst possible scenario, we’ll end up with 80K * 3 * 1.5M = 360 billion iterations. No wonder only this logic took almost an hour to complete.In the following sections, we’ll go through several optimization attempts. We should have the following considerations. We can’t get rid of the first loop since there is additional processing for each Part. So, that should remain intact. The original code finds the first match, not the best match. We should improve that too.Implementation 2In this first attempt we won’t utilize any drastically different algorithm. Let’s just explore how far we can push with the existing approach. Prepare the MasterPart dataset. Let’s create two additional arrays of it, one sorted by PartNumber and another one sorted by PartNumberNoHyphens. Remove duplicates and records with less than 3 characters. Now that we have a sorted collection, we don’t have to start looping from the first item. Perhaps we need a Dictionary&lt;int, int&gt; where the key is the length of the string, and the value is the starting position in the MasterPart collection. So, for each Part, we can retrieve the starting position to loop through MasterParts. There might be cases where a given length doesn’t exist in MasterParts, therefore, we have to fill these empty values in the dictionary with the next available length (since we’re searching for a suffix). All this state should be prepared before we start with the outer loop. As an example, this is what the dictionary will contain. The third rule we have has the opposite logic. We’re searching whether MasterPart.PartNumber is a suffix to Part.PartNumber. So we should fill the dictionaries accordingly and have to loop backwards. The EndsWith is already quite optimized in .NET 8. But, since we know all characters are ASCII, we can skip some of the checks and the switch statement in the EndsWidth implementation. Also, I found out that checking the first character before calling the SequenceEqual method improves the performance quite a bit. This is true in our scenario since we expect most of the checks to fail. We end up with the following method. [MethodImpl(MethodImplOptions.AggressiveInlining)]private static bool IsSuffix(ReadOnlySpan&lt;char&gt; source, ReadOnlySpan&lt;char&gt; suffix, int offset){ var segment = source.Slice(offset); return segment[0] == suffix[0] &amp;&amp; segment.SequenceEqual(suffix);} Use Span&lt;T&gt; and ReadOnlySpan&lt;T&gt; wherever possible. For looping, use For instead of ForEach and loop through ReadOnlySpan&lt;T&gt;.With all these changes we managed to decrease the time to ~9 minutes. It’s already a great improvement considering we didn’t apply any specific algorithm, we still have nested loops. It’s also worth mentioning that we also did improve the logic. Since the collection is sorted, we’ll find the best match first (e.g. strings are equal). The code for this implementation can be found here.Implementation 3I was curious how much improvement we’ll get if we just apply parallelization to the previous implementation. In our case, the only place where we can introduce parallelization is the outer loop. But, one of the requirements was to leave that intact, there is additional logic while iterating Parts that might need to be processed sequentially. What if we duplicate this loop? We loop the parts beforehand with Parallel.ForEach, build a final state Dictionary&lt;string, MasterPart?&gt;, and then in the original loop, we just check if there is a match in the dictionary.This decreased the time to 4 minutes, 2x improvement. We should be a bit careful with this approach and the way we interpret the results. I ran these benchmarks on a machine with 16 logical cores (8 physical) and with maximum parallelization. The CPU utilization was almost 100% and the machine was barely responsible during the run. The batch job in production might be scheduled in a host with 1-2 cores, and the overhead of parallelization might hurt the overall performance. The code for this implementation can be found here.Implementation 4By this point, it’s clear that if we want any further optimizations, we should adopt a brand-new approach. The real issue is that we have an O(n*m) operation (for the worst case), and we end up with 360 billion iterations. To make it clear, the iterations themselves are not the problem (that will happen within a second), but the operation under those iterations. The string comparison, in our case ReadOnlySpan&lt;char&gt;.SequenceEqual, requires some non-trivial compute time. Ideally, we should come up with an algorithm that doesn’t require string comparison at all. We need some sort of suffix lookup table. Create a final lookup state in the form of Dictionary&lt;string, MasterPart?&gt;, where the key is the Part.PartNumber. The original loop should contain a simple lookup check. Process and prepare the MasterParts collection in isolation, and build a suffix lookup table. We should end up with the following state Dictionary&lt;int, Dictionary&lt;string, MasterPart&gt;&gt;, where the key is the length of the string. The inner dictionary’s keys will represent all possible MasterPart suffixes for that given length. Utilize some of the techniques used in the previous implementations like start index tables while building this suffix lookup. For clarity, this is what the lookup state will contain for an example dataset. We end up with the following code for building the suffix lookup for MasterParts. At first glance, this might seem counterintuitive. We didn’t get rid of nested looping, it still exists, with itself. The difference is that the [^length..] operation is way simpler, it just slices the string.private static Dictionary&lt;int, Dictionary&lt;string, MasterPart&gt;&gt; GenerateDictionary( MasterPart[] masterPartNumbers, bool useNoHyphen){ var suffixesByLength = new Dictionary&lt;int, Dictionary&lt;string, MasterPart&gt;&gt;(51); var startIndexByLength = GenerateStartIndexesByLengthDictionary(masterPartNumbers, useNoHyphen); for (var length = 3; length &lt;= 50; length++) { var tempDictionary = new Dictionary&lt;string, MasterPart&gt;(); if (startIndexByLength.TryGetValue(length, out var startIndex) &amp;&amp; startIndex is not null) { for (var i = startIndex.Value; i &lt; masterPartNumbers.Length; i++) { var suffix = useNoHyphen ? masterPartNumbers[i].PartNumberNoHyphens[^length..] : masterPartNumbers[i].PartNumber[^length..]; tempDictionary.TryAdd(suffix, masterPartNumbers[i]); } } suffixesByLength[length] = tempDictionary; } return suffixesByLength;} Now that we have this in place, we can build the final state as follows. It’s worth noticing that the logic so far was just trying to add or retrieve records from a series of dictionaries.private static Dictionary&lt;string, MasterPart?&gt; BuildDictionary( MasterPartsInfo masterPartsInfo, PartsInfo partsInfo){ var masterPartsByPartNumber = new Dictionary&lt;string, MasterPart?&gt;(); for (var i = 0; i &lt; partsInfo.PartNumbers.Length; i++) { var partNumber = partsInfo.PartNumbers[i]; var match = FindMatchForPartNumber(partNumber, masterPartsInfo.SuffixesByLength); match ??= FindMatchForPartNumber(partNumber, masterPartsInfo.SuffixesByNoHyphensLength); if (match is not null) { masterPartsByPartNumber.TryAdd(partNumber, match); } } // Apply logic for the third rule and try add to the masterPartsByPartNumber dictionary. return masterPartsByPartNumber;}private static MasterPart? FindMatchForPartNumber( ReadOnlySpan&lt;char&gt; partNumber, Dictionary&lt;int, Dictionary&lt;string, MasterPart&gt;&gt; suffixByLength){ if (suffixByLength.TryGetValue(partNumber.Length, out var masterPartBySuffix) &amp;&amp; masterPartBySuffix != null) { masterPartBySuffix.TryGetValue(partNumber.ToString(), out var match); return match; } return null;} The third rule in the requirements contains the opposite condition, and I excluded that logic from the above snippets for brevity. We need to build the same suffix lookup for Parts too. But, since the final output should be a MasterPart, this proved to be a bit more challenging. The suffix lookup for Parts will have the following form Dictionary&lt;int, Dictionary&lt;string, List&lt;string&gt;&gt;&gt; where the List&lt;string&gt; is a collection of the original Part.PartNumber for a given suffix.This implementation completed the task in under 4 seconds. That includes all the initial processing, building the state, and looping through Parts. Everything is part of the benchmarks. The code can be found here.SummaryThe final results are shown below. We reduced the time from 2,527 seconds (~40 minutes) to just 4 seconds, over 600x speed improvement. This came with a different cost, memory utilization. It’s a tradeoff, and very rarely we can optimize for both speed and memory. That’s the case only for very poor original implementation. Anyhow, the client needed speed, and that was the goal. It’s a batch job, and once completed the process is terminated. So, having allocations is somewhat acceptable in this case. Also, this metric generally is a bit misunderstood. It has allocated 2GB of memory in the heap during the execution (and most of it collected by GC in our case). The final state we built is about 140MB. The 2GB is not a minimum memory requirement, but it will affect how much pressure we’ll have and how often the GC will kick in.I’m quite confident that this can be further optimized to run under 1 second. The trick is to know when to stop :). Working on performance improvements often can be quite rewarding, and very easily we can get stuck in an endless loop of optimization attempts. It’s crucial to know when this process stops being beneficial in the grand scheme of things and starts being an ego trip. While the original code was 4-5 lines and perhaps written in 5-10 minutes, the final implementation contains a couple of hundred LOC. I tried various approaches and algorithms (not only the ones presented here) and spent 2-3 days benchmarking and coming up with the final implementation. The 4 seconds were a big win for the client, and that’s where we stopped the journey.Here are a few suggestions and considerations for further improvements: Contiguous memory. In any performance optimization task, this is the number one thing to strive for. Our implementation produces a fragmented memory state, and we face a lot of CPU cache misses. I have a somewhat general idea of how would I write a more memory-friendly implementation using unsafe code, but frankly, not sure how I’d do that with safe constructs in C#. Perhaps using MemorySpan&lt;T&gt; as a state will be a plausible starting point. There are some well-known algorithms for these types of problems, like suffix trees and Trie. I tried these approaches but couldn’t succeed in performing better than our final implementation. This also comes back to the limitation of not using unsafe code, so I ended up with some rudimentary implementations. I’d love it if someone gave it a try. The implementation can be fine-tuned a bit more, especially the aspect of memory utilization. For example, the PartNumberNoHyphens may contain only the processed records that actually contained hyphens originally. Also, the Parts suffix lookup, instead of List&lt;string&gt; may hold List&lt;int&gt;, only the indices to the records in the Parts array. This would save tons of string allocations. While we prepare the initial MasterPart and Part arrays, we’re using LINQ. The OrderBy and especially Distinct LINQ methods are highly optimized in .NET and insanely fast. The hand-rolled implementation for Distinct was just a few percent faster. Regardless, avoiding LINQ would improve memory utilization. The code generally can be cleaned up a bit and have a more reduced form. I intentionally wrote it in a verbose way, so it’s easier to follow the algorithm. Not sure if I succeeded in that.The code and the benchmarks can be found in the following GitHub repository.Thank you for your attention and happy coding!" }, { "title": "Global soft delete in EF Core!", "url": "/posts/soft-delete/", "categories": "Software Development", "tags": "dotnet efcore", "date": "2023-11-25 12:00:00 +0100", "snippet": "In this article, I’ll describe how to implement soft delete in EF Core. We would like to apply a global configuration and not on a per-entity basis. The implementation consists of two parts, applyi...", "content": "In this article, I’ll describe how to implement soft delete in EF Core. We would like to apply a global configuration and not on a per-entity basis. The implementation consists of two parts, applying soft delete to the items while saving changes and configuring a global query filter to be applied to the queries.We’ll start by defining an interface ISoftDelete. The interface should be implemented by all entities that require this feature.public interface ISoftDelete{ public bool IsDeleted { get; }}Applying soft deleteEF Core is under active development and many new features have been added with each new version. I’ll provide not only the final implementation but also its evolution over time. With each new version, we had to update the soft-delete implementation to account for the new features and changes in EF Core.Option 1In the beginning, this was simple. We loop through all the deleted items in the tracker and update the state to Modified.public override Task&lt;int&gt; SaveChangesAsync(CancellationToken cancellationToken = default){ foreach (var entry in ChangeTracker.Entries&lt;ISoftDelete&gt;()) { if (entry.State != EntityState.Deleted) continue; entry.State = EntityState.Modified; entry.CurrentValues[nameof(ISoftDelete.IsDeleted)] = true; } return base.SaveChangesAsync(cancellationToken);}Option 2With the introduction of owned entity types, the above implementation no longer works. In the case of OwnsOne, the type might be mapped to the same parent table. Considering that owned types internally are designed as fully blown entities (with generated PK/FK shadow properties), then for each parent, we must find the owned types and change their state as well. Failing to do so will result in an exception.public override Task&lt;int&gt; SaveChangesAsync(CancellationToken cancellationToken = default){ foreach (var entry in ChangeTracker.Entries&lt;ISoftDelete&gt;()) { if (entry.State != EntityState.Deleted) continue; entry.State = EntityState.Modified; entry.CurrentValues[nameof(ISoftDelete.IsDeleted)] = true; var ownedEntries = entry.References .Where(x =&gt; x.TargetEntry is not null &amp;&amp; x.TargetEntry.Metadata.IsOwned()); foreach (var ownedEntry in ownedEntries) { if (ownedEntry.TargetEntry is not null) { ownedEntry.TargetEntry.State = EntityState.Modified; } } } return base.SaveChangesAsync(cancellationToken);}Option 3In Option 2, we didn’t have to account for OwnsMany since those entities won’t be mapped to the same table anyway. But, with the introduction of JSON columns in EF Core, that is possible now. The entry.References don’t include collections, and searching through entry.Collections or entry.Navigations is not straightforward either. So, we have to come up with an alternative approach. We’ll fetch all owned entries in the tracker, and then check whether they’re owned by a given deleted EntityEntry.public override Task&lt;int&gt; SaveChangesAsync(CancellationToken cancellationToken = default){ List&lt;EntityEntry&gt;? ownedEntries = null; foreach (var entry in ChangeTracker.Entries&lt;ISoftDelete&gt;()) { if (entry.State != EntityState.Deleted) continue; entry.State = EntityState.Modified; entry.CurrentValues[nameof(ISoftDelete.IsDeleted)] = true; ownedEntries ??= ChangeTracker.Entries() .Where(x =&gt; x.State == EntityState.Deleted &amp;&amp; x.Metadata.IsOwned()) .ToList(); foreach (var ownedEntry in ownedEntries) { if (ownedEntry.Metadata.IsInOwnershipPath(entry.Metadata)) { ownedEntry.State = EntityState.Modified; } } } return base.SaveChangesAsync(cancellationToken);}Configuring query filterThe EF Core offers a handy API to define a query filter for a given entity. All we need is to apply the following configuration.protected override void OnModelCreating(ModelBuilder modelBuilder){ modelBuilder.Entity&lt;Product&gt;().HasQueryFilter(x=&gt;x.IsDeleted);}Doing this for every entity is a tedious task. And we always run the risk of forgetting to add the configuration whenever we add a new entity. Instead, let’s try to implement this globally. We’ll create an extension as follows.public static class EFCoreExtensions{ private static readonly MethodInfo _softDeleteFilterMethodInfo = typeof(EFCoreExtensions) .GetMethod(nameof(GetSoftDeleteFilter), BindingFlags.NonPublic | BindingFlags.Static)!; private static LambdaExpression GetSoftDeleteFilter&lt;TEntity&gt;() where TEntity : class, ISoftDelete { Expression&lt;Func&lt;TEntity, bool&gt;&gt; filter = x =&gt; !x.IsDeleted; return filter; } public static void ConfigureSoftDelete(this ModelBuilder modelBuilder) { foreach (var entityType in modelBuilder.Model.GetEntityTypes()) { // In case we have inherited types, it's important to add it only for roots. var isRootType = entityType.GetRootType() == entityType; // Global query filters can not be assigned to owned entities, so we'll exclude them too. if (isRootType &amp;&amp; !entityType.IsOwned() &amp;&amp; typeof(ISoftDelete).IsAssignableFrom(entityType.ClrType)) { // The property is immutable (contains only a getter) // By default will be ignored by EF, so we need to add it to the model explicitly. modelBuilder.Entity(entityType.Name, x =&gt; x.Property(nameof(ISoftDelete.IsDeleted))); var methodToCall = _softDeleteFilterMethodInfo.MakeGenericMethod(entityType.ClrType); var filter = methodToCall.Invoke(null, Array.Empty&lt;object&gt;()); entityType.SetQueryFilter((LambdaExpression?)filter); entityType.AddIndex(entityType.FindProperty(nameof(ISoftDelete.IsDeleted))!); } } }}Now, we can apply the query filter for all entities (that implement the ISoftDelete interface) in our model.protected override void OnModelCreating(ModelBuilder modelBuilder){ modelBuilder.ConfigureSoftDelete();}I hope you found the article useful and happy coding!" }, { "title": "Current user implementation in ASP.NET Core!", "url": "/posts/current-user-aspnetcore/", "categories": "Software Development", "tags": "dotnet aspnetcore", "date": "2023-09-05 13:00:00 +0200", "snippet": "In this article, I’ll go through a couple of implementations on how to extract and provide user information in ASP.NET Core web applications. We apply authentication and authorization to protect ou...", "content": "In this article, I’ll go through a couple of implementations on how to extract and provide user information in ASP.NET Core web applications. We apply authentication and authorization to protect our endpoints, and usually, we utilize the user information throughout our services. That may be for logging purposes, auditing or other reasons. Fetching this data over and over again is neither performant nor convenient. Since this data doesn’t change per request, ideally, we’d like to extract it once and provide it in an immutable form to all our services.Let’s start by defining an interface that will expose user information.public interface ICurrentUser{ public string? UserId { get; } public string? Username { get; }}Option 1The most common approach I see in various solutions is to utilize the IHttpContextAccessor to extract the necessary data from the current request. This works, but it’s not ideal. It relies on AsyncLocal which can have a negative performance impact and creates a dependency on the “ambient state”. I try to avoid this interface as much as possible and use it only as a last resort. The implementation would be as follows.public class CurrentUser : ICurrentUser{ public string? UserId { get; } public string? Username { get; } public CurrentUser(IHttpContextAccessor httpContextAccessor) { var claimsPrincipal = httpContextAccessor.HttpContext?.User; if (claimsPrincipal is null) return; UserId = claimsPrincipal.FindFirstValue(ClaimTypes.NameIdentifier); UserId = claimsPrincipal.FindFirstValue(ClaimTypes.Name); }}Once we do the following registrations in DI, we may consume the ICurrentUser from all our services.builder.Services.AddHttpContextAccessor();builder.Services.AddScoped&lt;ICurrentUser, CurrentUser&gt;();Option 2A better approach would be to not utilize IHttpContextAccessor, instead, use a custom middleware to extract the information early in the request. Since our ICurrentUser is fully immutable, we’ll create an additional contract.public interface ICurrentUserInitializer{ public string? UserId { get; set; } public string? Username { get; set; }}public class CurrentUser : ICurrentUser, ICurrentUserInitializer{ public string? UserId { get; set; } public string? Username { get; set; }}Now, we go ahead and implement the middleware. The ICurrentUserInitializer is not mandatory to have. It’s just convenient for testing where we may inject a faker with pre-populated test user data. That’s the reason, we’re setting the user data only if they’re empty currentUser.UserId ??.public static class CurrentUserExtensions{ public static IApplicationBuilder UseCurrentUser(this IApplicationBuilder app) { app.Use(async (context, next) =&gt; { var user = context.User; var currentUser = context.RequestServices.GetRequiredService&lt;ICurrentUserInitializer&gt;(); currentUser.UserId ??= user.FindFirstValue(ClaimTypes.NameIdentifier); currentUser.Username ??= user.FindFirstValue(ClaimTypes.Name); await next(); }); return app; }}Finally, we need to wire up everything as follows....builder.Services.AddScoped&lt;CurrentUser&gt;();builder.Services.AddScoped&lt;ICurrentUserInitializer, CurrentUser&gt;(x =&gt; x.GetRequiredService&lt;CurrentUser&gt;());builder.Services.AddScoped&lt;ICurrentUser, CurrentUser&gt;(x =&gt; x.GetRequiredService&lt;CurrentUser&gt;());var app = builder.Build();...app.UseAuthorization();app.MapControllers();app.UseCurrentUser();app.Run();I hope you found the article useful and happy coding!" }, { "title": "Asynchronous programming in ASP.NET web stack!", "url": "/posts/asynchronous-programming-in-aspnet-web-stack/", "categories": "Software Development", "tags": "dotnet", "date": "2023-05-16 13:00:00 +0200", "snippet": "In .NET we have three patterns for asynchronous operations, Asynchronous Programming Model (APM), Event-based Asynchronous Pattern (EAP) and Task-based Asynchronous Pattern (TAP). Nowadays, the TAP...", "content": "In .NET we have three patterns for asynchronous operations, Asynchronous Programming Model (APM), Event-based Asynchronous Pattern (EAP) and Task-based Asynchronous Pattern (TAP). Nowadays, the TAP is ubiquitous and the recommended asynchronous pattern. The TAP model greatly simplifies the asynchronous calls with async/await, and that’s the way to go.Unfortunately, that’s not the case for some of the older platforms and frameworks. Since the TAP was not yet introduced at that time, there was no native support for it. Instead, there are some well-defined “workarounds” on how to make use of the async/await pattern in the older ASP.NET web applications. In this article, I’ll go through various components of ASP.NET 4.8 and describe how to use asynchronous code in them.ASP.NET MVC/WebApiThese are fairly new frameworks and there is native support for async/await. You don’t need to apply any special workaround.ASP.NET WebForms (aspx)Let’s say we have the following handler in a given aspx page.protected void Button1_Click(object sender, EventArgs e){ // Some logic var customer = _customerService.GetCustomer(1); TextBox1.Text = customer.Name; // Some other logic}The CustomerService also exposes an async method GetCustomerAsync, and we would like to utilize it in our handler. We need to apply the following changes: Open the markup of the page and add the Async=\"true\" directive. It would be something as shown below. &lt;%@ Page Async=\"true\" Title=\"Home Page\" Language=\"C#\" MasterPageFile=\"~/Site.Master\" AutoEventWireup=\"true\" CodeBehind=\"Default.aspx.cs\" Inherits=\"DemoApp.WebForms._Default\" %&gt; Create a new method with async Task signature and copy the whole content of the handler into it. You may adopt a convention where you’ll name these methods the same as the handlers, and then just add the Async suffix. In the original handler, you’ll use RegisterAsyncTask to register the newly created method for asynchronous execution.We’ll end up with the following code. This is all we need to do, so we can make async calls from our pages.protected void Button1_Click(object sender, EventArgs e){ RegisterAsyncTask(new PageAsyncTask(Button1_ClickAsync));}protected async Task Button1_ClickAsync(){ // Some logic var customer = await _customerService.GetCustomerAsync(1); TextBox1.Text = customer.Name; // Some other logic}What not to do Do not use async void for the handlers. In some platforms like WinForms/WPF we don’t have a choice, but for WebForms pages we have a better solution. So, try to adopt the above solution. Do not partially extract the content of the handlers to the new async method. We have to understand that with RegisterAsyncTask we’re just registering the action for asynchronous execution, but the handler is still a void method, and it won’t wait for the execution to complete. So, any code that you have after RegisterAsyncTask will be executed immediately. Avoid adding any code after it, unless you’re completely sure that it is not dependent on the results or side effects of the async method.ASP.NET HttpHandlers (ashx)If you have a lot of HTTP handlers in the solution, you might need to execute async methods in them. Luckily, it’s quite easy to do that for HTTP handlers. All we need is to inherit from HttpTaskAsyncHandler abstract base class.Let’s say we have the following handler.public class CustomerHandler : IHttpHandler{ public void ProcessRequest(HttpContext context) { var customerService = new CustomerService(); var customer = customerService.GetCustomer(1); context.Response.ContentType = \"text/plain\"; context.Response.Write(customer.Name); } public bool IsReusable { get { return false; } }}If we want to utilize the GetCustomerAsync method, we’ll update the handler as follows:public class CustomerHandler : HttpTaskAsyncHandler{ public override async Task ProcessRequestAsync(HttpContext context) { var customerService = new CustomerService(); var customer = await customerService.GetCustomerAsync(1); context.Response.ContentType = \"text/plain\"; context.Response.Write(customer.Name); }}ASP.NET Web Services (asmx)This is the trickiest one. The web services (asmx) don’t have support for the TAP model (async/await), but they do have support for APM. The APM was the first asynchronous model introduced in .NET Framework 1.0. In this pattern, asynchronous operations require Begin and End methods (for example, BeginSomething and EndSomething to implement an asynchronous operation). It’s a legacy model and no longer recommended. We can write a TAP to APM interop extensions, so we can consume the Task-based methods from our web services.Let’s say we have the following web service.public class CustomerWebService : System.Web.Services.WebService{ [WebMethod] public string GetCustomerName(int id) { var customerService = new CustomerService(); var customerName = customerService.GetCustomerName(id); return customerName; }}If we want to utilize the async methods in CustomerService, we’ll have to make the following changes.public class CustomerWebService : System.Web.Services.WebService{ [WebMethod] public IAsyncResult BeginGetCustomerName(int id, AsyncCallback callback, object state) { var customerService = new CustomerService(); return customerService.GetCustomerNameAsync(id).AsApm(callback, state); } [WebMethod] public string EndGetCustomerName(IAsyncResult result) { return result.Unwrap&lt;string&gt;(); }}The APM interop implementation is as follows.public static class ApmInteropExtensions{ public static IAsyncResult AsApm(this Task task, AsyncCallback callback, object state) { if (task == null) { throw new ArgumentNullException(nameof(task)); } var tcs = new TaskCompletionSource&lt;object&gt;(state); task.ContinueWith(t =&gt; { if (t.IsFaulted) { tcs.TrySetException(t.Exception.InnerExceptions); } else if (t.IsCanceled) { tcs.TrySetCanceled(); } else { tcs.TrySetResult(null); } if (callback != null) { callback(tcs.Task); } }, TaskScheduler.Default); return tcs.Task; } public static IAsyncResult AsApm&lt;T&gt;(this Task&lt;T&gt; task, AsyncCallback callback, object state) { if (task == null) { throw new ArgumentNullException(nameof(task)); } var tcs = new TaskCompletionSource&lt;T&gt;(state); task.ContinueWith(t =&gt; { if (t.IsFaulted) { tcs.TrySetException(t.Exception.InnerExceptions); } else if (t.IsCanceled) { tcs.TrySetCanceled(); } else { tcs.TrySetResult(t.Result); } if (callback != null) { callback(tcs.Task); } }, TaskScheduler.Default); return tcs.Task; } public static void Unwrap(this IAsyncResult asyncResult) { if (asyncResult == null) { throw new ArgumentNullException(nameof(asyncResult)); } if (asyncResult is Task task) { task.GetAwaiter().GetResult(); } else { throw new ArgumentException(\"Invalid asyncResult\", nameof(asyncResult)); } } public static T Unwrap&lt;T&gt;(this IAsyncResult asyncResult) { if (asyncResult == null) { throw new ArgumentNullException(nameof(asyncResult)); } if (asyncResult is Task&lt;T&gt; task) { return task.GetAwaiter().GetResult(); } else { throw new ArgumentException(\"Invalid asyncResult\", nameof(asyncResult)); } }}Let’s summarize what we have done here: For a given web method, we have created two methods with the same name. Added Begin and End prefixes to the name of the methods. The Begin method will accept the original input parameters, and additionally the AsyncCallBack and state (these will be provided by the framework). It returns IAsyncResult. The End method will accept IAsyncResult (provided by the framework) and will return the original output.Note: The EndGetCustomerName won’t run on the original thread and it won’t have access to the original context. It means you won’t have access to HttpContext.Current. Bear that in mind and avoid some complex logic in End methods. Its job is to just extract and return the result.I hope you found the article useful and happy coding!" }, { "title": "Simplifying Configuration for Temporal Tables in EF Core!", "url": "/posts/simplifying-configuration-for-temporal-tables-in-EF-Core/", "categories": "Blogging, Tutorial, Software Development", "tags": "EFCore", "date": "2023-01-15 12:00:00 +0100", "snippet": "When dealing with data that changes over time, you may want to keep track of historical changes. Temporal tables in SQL Server offer an excellent solution to this problem, allowing you to automatic...", "content": "When dealing with data that changes over time, you may want to keep track of historical changes. Temporal tables in SQL Server offer an excellent solution to this problem, allowing you to automatically track changes to your data. With temporal tables, you can see what your data looked like at any point in time, making it easier to debug issues and track changes over time. Starting with Entity Framework Core 6, the team added support for this feature. In this blog post, we will discuss how to create an extension method for EF Core that automatically configures temporal tables for all entities of interest.Configuring temporal tables for simple entitiesThe required configuration is quite straightforward. Let’s assume we have the following model/entity.public class Customer{ public int Id { get; set; } public string? Name { get; set; }}The required configuration is as follows:protected override void OnModelCreating(ModelBuilder modelBuilder){ modelBuilder.Entity&lt;Customer&gt;().ToTable(x =&gt; x.IsTemporal());}That’s all we need to configure a temporal table for Customer. The EF will automatically generate shadow properties PeriodStart and PeriodEnd for our model. If you’re using migrations, the following SQL will be generatedDECLARE @historyTableSchema sysname = SCHEMA_NAME()EXEC(N'CREATE TABLE [Customers] ( [Id] int NOT NULL IDENTITY, [Name] nvarchar(max) NULL, [PeriodEnd] datetime2 GENERATED ALWAYS AS ROW END HIDDEN NOT NULL, [PeriodStart] datetime2 GENERATED ALWAYS AS ROW START HIDDEN NOT NULL, CONSTRAINT [PK_Customers] PRIMARY KEY ([Id]), PERIOD FOR SYSTEM_TIME([PeriodStart], [PeriodEnd])) WITH (SYSTEM_VERSIONING = ON (HISTORY_TABLE = [' + @historyTableSchema + N'].[CustomersHistory]))');Configuring temporal tables for entities with owned entity typesIn EF Core 6 we lacked support for this feature if the entity has defined owned types mapped to the same table. This was added in EF Core 7. But, configuring them is not intuitive nor simple. Let’s assume we have the following model nowpublic class Address{ public string? Street { get; set; } public string? City { get; set; }}public class Customer{ public int Id { get; set; } public string? Name { get; set; } public Address? Address { get; set; }}We assume that the configuration might be as followsprotected override void OnModelCreating(ModelBuilder modelBuilder){ modelBuilder.Entity&lt;Customer&gt;().ToTable(x =&gt; x.IsTemporal()); modelBuilder.Entity&lt;Customer&gt;().OwnsOne(x =&gt; x.Address, ownedBuilder =&gt; { ownedBuilder.ToTable(x =&gt; x.IsTemporal()); });}Unfortunately, this will result in an exception while building the model.System.InvalidOperationException HResult=0x80131509 Message=When multiple temporal entities are mapped to the same table, their period start properties must map to the same column. Issue happens for entity type 'Customer' with period property 'PeriodStart' which is mapped to column 'PeriodStart'. Expected period column name is 'Address_PeriodStart'.There is some indication that the generated column names for shadow properties of the owned entities is incorrect. The first guess will be to just define the column names for the owned entity only. But, weirdly, that’s not enough. In this case we have to be fully explicit as follows:protected override void OnModelCreating(ModelBuilder modelBuilder){ modelBuilder.Entity&lt;Customer&gt;().ToTable(x =&gt; x.IsTemporal()); modelBuilder.Entity&lt;Customer&gt;().Property&lt;DateTime&gt;(\"PeriodStart\").HasColumnName(\"PeriodStart\"); modelBuilder.Entity&lt;Customer&gt;().Property&lt;DateTime&gt;(\"PeriodEnd\").HasColumnName(\"PeriodEnd\"); modelBuilder.Entity&lt;Customer&gt;().OwnsOne(x =&gt; x.Address, ownedBuilder =&gt; { ownedBuilder.ToTable(x =&gt; x.IsTemporal()); ownedBuilder.Property&lt;DateTime&gt;(\"PeriodStart\").HasColumnName(\"PeriodStart\"); ownedBuilder.Property&lt;DateTime&gt;(\"PeriodEnd\").HasColumnName(\"PeriodEnd\"); });}Finally, this works, and the generated SQL is correctDECLARE @historyTableSchema sysname = SCHEMA_NAME()EXEC(N'CREATE TABLE [Customers] ( [Id] int NOT NULL IDENTITY, [Name] nvarchar(max) NULL, [Address_Street] nvarchar(max) NULL, [Address_City] nvarchar(max) NULL, [PeriodEnd] datetime2 GENERATED ALWAYS AS ROW END HIDDEN NOT NULL, [PeriodStart] datetime2 GENERATED ALWAYS AS ROW START HIDDEN NOT NULL, CONSTRAINT [PK_Customers] PRIMARY KEY ([Id]), PERIOD FOR SYSTEM_TIME([PeriodStart], [PeriodEnd])) WITH (SYSTEM_VERSIONING = ON (HISTORY_TABLE = [' + @historyTableSchema + N'].[CustomersHistory]))');Applying temporal table configuration globallyAs shown above, the configuration is a bit overwhelming. Doing it for all your entities is a tedious task. Luckily, we can extend the ModelBuilder capabilities and write an extension method that does this automatically for all entities. Ideally, we’d like to to end up with the following configuration.protected override void OnModelCreating(ModelBuilder modelBuilder){ modelBuilder.Entity&lt;Customer&gt;().OwnsOne(x =&gt; x.Address); // Call the method at the end after all other configurations. modelBuilder.ConfigureTemporalTables();}We also may want the ability to choose which entities should be taken into consideration. Therefore, we may add an interface that will mark all the entities that should be tracked with temporal tables.public interface ITemporalEntity{}public class Address{ public string? Street { get; set; } public string? City { get; set; }}public class Customer : ITemporalEntity{ public int Id { get; set; } public string? Name { get; set; } public Address? Address { get; set; }}public static class ModelBuilderExtensions{ public static void ConfigureTemporalTables(this ModelBuilder modelBuilder) { foreach (var entityType in modelBuilder.Model.GetEntityTypes()) { if (entityType.IsOwned()) { if (!entityType.IsMappedToJson()) { ConfigureTemporalForOwnedType(entityType, entityType); } } else { if (typeof(ITemporalEntity).IsAssignableFrom(entityType.ClrType)) { ConfigureTemporal(entityType); } } } } private static void ConfigureTemporalForOwnedType(IMutableEntityType entityType, IMutableEntityType parentEntityType) { var ownership = parentEntityType.FindOwnership(); if (ownership is null) return; var parent = ownership.PrincipalEntityType; if (parent is null) return; if (parent.IsOwned()) { ConfigureTemporalForOwnedType(entityType, parent); } else if (typeof(ITemporalEntity).IsAssignableFrom(parent.ClrType)) { ConfigureTemporal(entityType); } } private static void ConfigureTemporal(IMutableEntityType entityType) { entityType.SetIsTemporal(true); var periodStart = entityType.FindDeclaredProperty(\"PeriodStart\"); if (periodStart is not null) periodStart.SetColumnName(\"PeriodStart\"); var periodEnd = entityType.FindDeclaredProperty(\"PeriodEnd\"); if (periodEnd is not null) periodEnd.SetColumnName(\"PeriodEnd\"); }}You may notice, we don’t even have to mark our owned types with the interface, they’ll be discovered automatically. Marking the parent entities is enough.So, that’s it, we’re all set. I hope this is helpful!" }, { "title": "Publishing strategies in MediatR", "url": "/posts/publishing-strategies-in-MediatR/", "categories": "Blogging, Tutorial, Software Development", "tags": "Mediator", "date": "2022-12-17 12:00:00 +0100", "snippet": " EDIT: There are breaking changes in MediatR 12.0.1, and the following implementation won’t work. For v12, please check the new article here.The Mediator pattern is a behavioral design pattern that...", "content": " EDIT: There are breaking changes in MediatR 12.0.1, and the following implementation won’t work. For v12, please check the new article here.The Mediator pattern is a behavioral design pattern that promotes loose coupling between objects by having a central point of communication, which is called the mediator. This pattern is particularly useful when you have a complex system with multiple interacting components that need to communicate with each other. By introducing a mediator object, you can reduce the dependencies between these components, making it easier to maintain and evolve the system over time. In the Mediator pattern, components don’t interact with each other directly; instead, they send messages to the mediator, which then coordinates the communication between the components. This way, components can be added, removed, or modified without affecting other parts of the system. The mediator’s primary responsibility is to facilitate the interaction between the components and to ensure that they collaborate correctly.One of the popular and widely used implementation in .NET is the MediatR library. It has support for various scenarios, including request/response, commands, queries, and notifications. In this article we’ll focus on notifications and how we can utilize different publishing strategies. The library by default calls and await each handler sequentially. In case of an exception, the execution is stopped, meaning the rest of the handlers won’t execute. We’d like to extend this behavior, and use a different strategy on demand. First, let’s define our requirements. Implement multiple publishing strategies We should be able to choose a strategy while publishing a notification. Ideally, the feature should be an extension to IMediator. Consumers should not have to deal with new types.Publishing StrategiesLet’s first create an enum with the strategies we plan on supporting.public enum PublishStrategy{ /// &lt;summary&gt; /// Run each notification handler after one another. Returns when all handlers are finished or an exception has been thrown. In case of an exception, any handlers after that will not be run. /// &lt;/summary&gt; AsyncSequentialStopOnException = 0, /// &lt;summary&gt; /// Run each notification handler after one another. Returns when all handlers are finished. In case of any exception(s), they will be captured in an AggregateException. /// &lt;/summary&gt; AsyncSequentialContinueOnException = 1, /// &lt;summary&gt; /// Run all notification handlers asynchronously. Returns when all handlers are finished. In case of any exception(s), they will be captured in an AggregateException. /// &lt;/summary&gt; AsyncWhenAll = 2, /// &lt;summary&gt; /// Run each notification handler on its own thread using Task.Run(). Returns when all threads (handlers) are finished. In case of any exception(s), if the call to Publish is awaited, they are captured in an AggregateException by Task.WhenAll. Do not use this strategy if you're accessing the database in your handlers, DbContext is not thread-safe. /// &lt;/summary&gt; ParallelWhenAll = 3, /// &lt;summary&gt; /// Create a single new thread using Task.Run(), and run all notifications sequentially (continue on exception). Returns immediately and does not wait for any handlers to finish. Note that you cannot capture any exceptions, even if you await the call to Publish. To improve the traceability the exception is being captured internally and logged with ILogger if available. /// &lt;/summary&gt; AsyncNoWait = 4, /// &lt;summary&gt; /// Run each notification handler on its own thread using Task.Run(). Returns immediately and does not wait for any handlers to finish. Note that you cannot capture any exceptions, even if you await the call to Publish. To improve the traceability the exception is being captured internally and logged with ILogger if available. Do not use this strategy if you're accessing the database in your handlers, DbContext is not thread-safe. /// &lt;/summary&gt; ParallelNoWait = 5,}Extending Mediator ImplementationNext, let’s define a custom mediator implementationpublic class ExtendedMediator : Mediator{ private readonly ServiceFactory _serviceFactory; private readonly IServiceScopeFactory _serviceScopeFactory; private readonly Func&lt;IEnumerable&lt;Func&lt;INotification, CancellationToken, Task&gt;&gt;, INotification, CancellationToken, Task&gt; _publish; private ExtendedMediator( ServiceFactory serviceFactory, Func&lt;IEnumerable&lt;Func&lt;INotification, CancellationToken, Task&gt;&gt;, INotification, CancellationToken, Task&gt; publish) : base(serviceFactory) { _serviceFactory = serviceFactory; _serviceScopeFactory = default!; _publish = publish; } public ExtendedMediator(ServiceFactory serviceFactory, IServiceScopeFactory serviceScopeFactory) : base(serviceFactory) { _serviceFactory = serviceFactory; _serviceScopeFactory = serviceScopeFactory; _publish = base.PublishCore; } protected override Task PublishCore( IEnumerable&lt;Func&lt;INotification, CancellationToken, Task&gt;&gt; allHandlers, INotification notification, CancellationToken cancellationToken) { return _publish(allHandlers, notification, cancellationToken); } public Task Publish&lt;TNotification&gt;( TNotification notification, PublishStrategy strategy, CancellationToken cancellationToken) where TNotification : INotification { return strategy switch { PublishStrategy.AsyncNoWait =&gt; PublishNoWait(_serviceScopeFactory, notification, AsyncSequentialContinueOnException, cancellationToken), PublishStrategy.ParallelNoWait =&gt; PublishNoWait(_serviceScopeFactory, notification, ParallelWhenAll, cancellationToken), PublishStrategy.AsyncSequentialContinueOnException =&gt; new ExtendedMediator(_serviceFactory, AsyncSequentialContinueOnException).Publish(notification, cancellationToken), PublishStrategy.AsyncSequentialStopOnException =&gt; new ExtendedMediator(_serviceFactory, AsyncSequentialStopOnException).Publish(notification, cancellationToken), PublishStrategy.AsyncWhenAll =&gt; new ExtendedMediator(_serviceFactory, AsyncWhenAll).Publish(notification, cancellationToken), PublishStrategy.ParallelWhenAll =&gt; new ExtendedMediator(_serviceFactory, ParallelWhenAll).Publish(notification, cancellationToken), _ =&gt; throw new ArgumentException($\"Unknown strategy: {strategy}\") }; } private static Task PublishNoWait( IServiceScopeFactory serviceScopeFactory, INotification notification, Func&lt;IEnumerable&lt;Func&lt;INotification, CancellationToken, Task&gt;&gt;, INotification, CancellationToken, Task&gt; publish, CancellationToken cancellationToken) { _ = Task.Run(async () =&gt; { using var scope = serviceScopeFactory.CreateScope(); var logger = scope.ServiceProvider.GetService&lt;ILogger&lt;ExtendedMediator&gt;&gt;(); try { var mediator = new ExtendedMediator(scope.ServiceProvider.GetRequiredService, publish); await mediator.Publish(notification, cancellationToken).ConfigureAwait(false); } catch (Exception ex) { if (logger is not null) { logger.LogError(ex, \"Error occurred while executing the handler in NoWait mode\"); } } }, cancellationToken); return Task.CompletedTask; } private static Task ParallelWhenAll( IEnumerable&lt;Func&lt;INotification, CancellationToken, Task&gt;&gt; handlers, INotification notification, CancellationToken cancellationToken) { var tasks = new List&lt;Task&gt;(); foreach (var handler in handlers) { tasks.Add(Task.Run(() =&gt; handler(notification, cancellationToken), cancellationToken)); } return Task.WhenAll(tasks); } private static async Task AsyncWhenAll( IEnumerable&lt;Func&lt;INotification, CancellationToken, Task&gt;&gt; handlers, INotification notification, CancellationToken cancellationToken) { var tasks = new List&lt;Task&gt;(); var exceptions = new List&lt;Exception&gt;(); foreach (var handler in handlers) { try { tasks.Add(handler(notification, cancellationToken)); } catch (Exception ex) when (!(ex is OutOfMemoryException || ex is StackOverflowException)) { exceptions.Add(ex); } } try { await Task.WhenAll(tasks).ConfigureAwait(false); } catch (AggregateException ex) { exceptions.AddRange(ex.Flatten().InnerExceptions); } catch (Exception ex) when (!(ex is OutOfMemoryException || ex is StackOverflowException)) { exceptions.Add(ex); } if (exceptions.Any()) { throw new AggregateException(exceptions); } } private static async Task AsyncSequentialContinueOnException( IEnumerable&lt;Func&lt;INotification, CancellationToken, Task&gt;&gt; handlers, INotification notification, CancellationToken cancellationToken) { var exceptions = new List&lt;Exception&gt;(); foreach (var handler in handlers) { try { await handler(notification, cancellationToken).ConfigureAwait(false); } catch (AggregateException ex) { exceptions.AddRange(ex.Flatten().InnerExceptions); } catch (Exception ex) when (!(ex is OutOfMemoryException || ex is StackOverflowException)) { exceptions.Add(ex); } } if (exceptions.Any()) { throw new AggregateException(exceptions); } } private static async Task AsyncSequentialStopOnException( IEnumerable&lt;Func&lt;INotification, CancellationToken, Task&gt;&gt; handlers, INotification notification, CancellationToken cancellationToken) { foreach (var handler in handlers) { await handler(notification, cancellationToken).ConfigureAwait(false); } }}Registration and ExtensionsWe’ll define few extension methods to help the registration and create an overload for the Publish method.public static class MediatorExtensions{ public static Task Publish&lt;TNotification&gt;(this IMediator mediator, TNotification notification, PublishStrategy strategy, CancellationToken cancellationToken) where TNotification : INotification { if (mediator is ExtendedMediator customMediator) { return customMediator.Publish(notification, strategy, cancellationToken); } throw new NotSupportedException(\"The custom mediator implementation is not registered!\"); } public static IServiceCollection AddExtendedMediatR(this IServiceCollection services, params Assembly[] assemblies) { services.AddMediatR(options =&gt; options.Using&lt;ExtendedMediator&gt;().AsScoped(), assemblies); return services; } public static IServiceCollection AddExtendedMediatR(this IServiceCollection services, params Type[] handlerAssemblyMarkerTypes) { services.AddMediatR(options =&gt; options.Using&lt;ExtendedMediator&gt;().AsScoped(), handlerAssemblyMarkerTypes); return services; }}UsageNow that we’re all set, the usage is quite straightforward. Register the extended mediator.builder.Services.AddExtendedMediatR(typeof(Program));Use it using existing IMediator contract.[ApiController]public class DummyController : ControllerBase{ private readonly IMediator _mediator; public DummyController(IMediator mediator) { _mediator = mediator; } [HttpGet(\"/\")] public async Task&lt;ActionResult&gt; Get(CancellationToken cancellationToken) { await _mediator.Publish(new Ping(), PublishStrategy.AsyncNoWait, cancellationToken); return Ok(); }}I hope you found this article useful. Happy coding!" }, { "title": "Owned Entity Types: Ensuring Non-Nullable Navigation in EF Core", "url": "/posts/ensuring-non-nullable-navigation-in-ef-core/", "categories": "Blogging, Tutorial, Software Development", "tags": "EFCore", "date": "2022-11-30 12:00:00 +0100", "snippet": "In this article I will explore the possibility of a subtle bug you may face in your application, if you’re using owned entity types and you have incorrect EF configuration. Let’s start with an exam...", "content": "In this article I will explore the possibility of a subtle bug you may face in your application, if you’re using owned entity types and you have incorrect EF configuration. Let’s start with an example directly. We have the following models and the corresponding EF configuration. To demonstrate the issue more clearly I disabled the NRT (nullable reference types) in my project. Later, we’ll discuss how EF Core can automatically infer our model more accurately if we use NRT.public class Address{ public string Street { get; set; } public string City { get; set; }}public class Customer{ public int Id { get; private set; } public string Name { get; set; } public Address Address { get; private set; } private Customer() { // Required by EF } public Customer(Address address) { if (address == null) throw new ArgumentNullException(nameof(address)); Address = address; }}protected override void OnModelCreating(ModelBuilder modelBuilder){\tmodelBuilder.Entity&lt;Customer&gt;().OwnsOne(x =&gt; x.Address);}In this example, we have a Customer entity which contains an owned type Address. Based on the requirements we have received, the Customer.Name, Address.Street and Address.City properties may contain null values. This will correctly be reflected in the generated table, and the respective database columns will be marked as nullable. But, while designing our model, we have ensured that the Address in Customer should never be null. The state in Address may be null, but the navigation not. We may have various reasons why we would want that, but that’s out of scope here. We’re happy with the model, and we’re confident that we did a good job reflecting the requirements correctly in the code. The first hint that something is wrong is the warning message that is generated by EF while building the model. It’s generated on runtime or if you try to create a migration.The entity type 'Address' is an optional dependent using table sharing without any required non shared property that could be used to identify whether the entity exists. If all nullable properties contain a null value in database then an object instance won't be created in the query. Add a required property to create instances with null values for other properties or mark the incoming navigation as required to always create an instance.The message is clear, but yet a bit confusing. What it means is that since our owned type doesn’t have any non-nullable properties, and if all of them are null for a given record; while fetching that record the navigation itself will be null. The assumption we made that Address will never be null is not correct. We never told to EF that it’s required. The issue is easily replicated as shown in the code below. If we fetch the record from the database, the customer2 will have null value for Address.var customer1 = new Customer(new Address { Street = \"Street1\", City = \"City1\" });var customer2 = new Customer(new Address());dbContext.Customers.AddRange(customer1, customer2);await dbContext.SaveChangesAsync();dbContext.ChangeTracker.Clear();var customers = await dbContext.Customers.ToListAsync();This can be easily fixed if we’re explicit in the configuration and set the navigation as required.protected override void OnModelCreating(ModelBuilder modelBuilder){ modelBuilder.Entity&lt;Customer&gt;().OwnsOne(x =&gt; x.Address); modelBuilder.Entity&lt;Customer&gt;().Navigation(x =&gt; x.Address).IsRequired();}This may seem as a mundane issue, but in my experience is a very common misconfiguration. If you have NRT enabled in your projects you’ll be conditionally safe. If you have defined the property as non-nullable public Address Address { get; private set; }, EF will be able to deduce your intentions and mark the navigation as required automatically. Anyhow, I think it’s important to understand the underlying issue. Moreover, in case you have nested owned types, EF no longer will generate a warning, but an InvalidOperationException exception on runtime.With that in mind, let’s play safe and write an extension method that configure all owned type navigations as required automatically.protected override void OnModelCreating(ModelBuilder modelBuilder){ modelBuilder.Entity&lt;Customer&gt;().Navigation(x =&gt; x.Address).IsRequired(); // Call the method after all other configurations modelBuilder.ConfigureOwnedTypeNavigationsAsRequired();}public static partial class ModelBuilderExtensions{ public static void ConfigureOwnedTypeNavigationsAsRequired(this ModelBuilder modelBuilder) { foreach (var entityType in modelBuilder.Model.GetEntityTypes()) { if (entityType.IsOwned()) { var ownership = entityType.FindOwnership(); if (ownership is null) return; if (ownership.IsUnique) { ownership.IsRequiredDependent = true; } } } }}I hope you found this article useful. Happy coding!" }, { "title": "Customizing Column Naming Conventions for Owned Types in EF Core", "url": "/posts/customizing-column-naming-conventions-for-owned-types-in-ef-core/", "categories": "Blogging, Tutorial, Software Development", "tags": "EFCore", "date": "2022-10-13 13:00:00 +0200", "snippet": "EF Core allows you to model entity types that can only ever appear on navigation properties of other entity types. These are called owned entity types. By default, the owned entity types are mapped...", "content": "EF Core allows you to model entity types that can only ever appear on navigation properties of other entity types. These are called owned entity types. By default, the owned entity types are mapped to the same table as the top-level parent. You can explicitly change this behavior in the configuration. Since the parent and owned types may contain properties with the same name, the generated column names for the owned types will contain the navigation name as a prefix by default. Let’s see this in action.public class Foo{ public string? Bar { get; set; }}public class Address{ public string? Street { get; set; } public string? City { get; set; } public Foo Foo { get; set; }}public class Customer{ public int Id { get; set; } public string? Name { get; set; } public Address Address { get; set; }}protected override void OnModelCreating(ModelBuilder modelBuilder){ modelBuilder.Entity&lt;Customer&gt;().OwnsOne(x =&gt; x.Address, addressBuilder =&gt; addressBuilder.OwnsOne(x =&gt; x.Foo));}For the above model and corresponding configuration, the generated SQL statement is as follows.CREATE TABLE [Customers] ( [Id] int NOT NULL IDENTITY, [Name] nvarchar(max) NULL, [Address_Street] nvarchar(max) NULL, [Address_City] nvarchar(max) NULL, [Address_Foo_Bar] nvarchar(max) NULL, CONSTRAINT [PK_Customers] PRIMARY KEY ([Id]));Although this convention is necessary to avoid name conflicts, many teams dislike prefixing the column names. A more compelling argument is when you’re trying to match your EF model to an existing database. We can easily achieve that by explicitly setting the column names.protected override void OnModelCreating(ModelBuilder modelBuilder){ modelBuilder.Entity&lt;Customer&gt;().OwnsOne(x =&gt; x.Address, addressBuilder =&gt; { addressBuilder.Property(x =&gt; x.Street).HasColumnName(\"Address\"); addressBuilder.Property(x =&gt; x.City).HasColumnName(\"City\");\t\t addressBuilder.OwnsOne(x =&gt; x.Foo, fooBuilder =&gt; { fooBuilder.Property(x =&gt; x.Bar).HasColumnName(\"Bar\"); }); });}As you may imagine, for large codebases with many entities, overriding all column names might be a tedious task. Luckily, we can configure this convention globally for our EF model.protected override void OnModelCreating(ModelBuilder modelBuilder){ modelBuilder.Entity&lt;Customer&gt;().OwnsOne(x =&gt; x.Address, addressBuilder =&gt; addressBuilder.OwnsOne(x =&gt; x.Foo)); // Call the method at the end, after all other configurations modelBuilder.ConfigureOwnedTypeColumnNames();}public static class ModelBuilderExtensions{ public static void ConfigureOwnedTypeColumnNames(this ModelBuilder modelBuilder) { foreach (var entityType in modelBuilder.Model.GetEntityTypes()) { if (!entityType.IsOwned()) continue; var ownership = entityType.FindOwnership(); if (ownership is null) continue; var properties = entityType.GetProperties().Where(x =&gt; !x.IsShadowProperty()); foreach (var property in properties) { var tableName = entityType.GetTableName(); if (tableName is null) continue; var columnName = property.GetColumnName(StoreObjectIdentifier.Table(tableName, null)); var columnNameDefault = property.GetDefaultColumnName(StoreObjectIdentifier.Table(tableName, null)); if (columnName is null || columnNameDefault is null) continue; if (columnName.Equals(columnNameDefault)) { var columnNameBase = property.GetColumnName(); property.SetColumnName(columnNameBase); } } } }}Once we apply this configuration, the generarated SQL statement matches our desired convention.CREATE TABLE [Customers] ( [Id] int NOT NULL IDENTITY, [Name] nvarchar(max) NULL, [Street] nvarchar(max) NULL, [City] nvarchar(max) NULL, [Bar] nvarchar(max) NULL, CONSTRAINT [PK_Customers] PRIMARY KEY ([Id]));I hope you found this article useful. Happy coding!" }, { "title": "Install wkhtmltopdf for Azure Linux App Service Plans", "url": "/posts/install-wkhtmltopdf-for-azure-linux-app-service-plans/", "categories": "Blogging, Tutorial, Software Development", "tags": "Azure", "date": "2022-09-18 13:00:00 +0200", "snippet": "When working with web applications, you may occasionally need to convert HTML content into images or PDFs. In my case, I stumbled upon this requirement and, after a quick search, I discovered a fan...", "content": "When working with web applications, you may occasionally need to convert HTML content into images or PDFs. In my case, I stumbled upon this requirement and, after a quick search, I discovered a fantastic open-source package called wkhtmltopdf. This powerful package provides an efficient solution for such conversions, offering two command-line tools: wkhtmltopdf for converting HTML to PDF, and wkhtmltoimage for converting HTML to various image formats. You can support the authors here: https://wkhtmltopdf.org/.While the wkhtmltopdf package is conveniently included in Windows plans by default, I ran into a challenge when I switched to a Linux plan. My application began throwing exceptions, and I soon realized that the tools were not pre-installed on Linux hosts. Since Azure spins up a new host/container with each application start and any data outside the /home directory is not persisted, just installing the package is not enough. This article will walk you through the steps to install wkhtmltopdf on Azure Linux App Service Plans during app startup, ensuring seamless integration with your application. Login to Azure Portal Select your App Service instance. Choose the SSH menu and then open the SSH console. Create two files, install-tools.sh and startup.sh, under the /home/site/wwwroot/ directory. Follow the steps from the screenshot below: install-tools.sh apt-get updateapt-get install -y wkhtmltopdf startup.sh sh /home/site/wwwroot/install-tools.sh &amp;dotnet DemoApp.dll Go to your App Service instance again. Choose the Configuration menu, the General settings sub-menu, and update the startup command to ./startup.sh. Save the changes. In the Overview section of the same page, stop and start the service. Your application should be up and running immediately. After some time (it may take a couple of minutes), connect to the SSH console again and check whether the tool is available. I hope you found this article useful. Happy coding!" }, { "title": "Setting Global Table Naming Conventions in EF Core", "url": "/posts/setting-global-table-naming-conventions-in-EF-Core/", "categories": "Blogging, Tutorial, Software Development", "tags": "EFCore", "date": "2022-07-20 13:00:00 +0200", "snippet": "I was recently asked about configuring a default convention in EF Core so that the generated table names use the entity names. Additionally, if a table name is set explicitly for a given entity (th...", "content": "I was recently asked about configuring a default convention in EF Core so that the generated table names use the entity names. Additionally, if a table name is set explicitly for a given entity (through the ToTable() method in entity configuration), it should override the convention. Initially, I thought this would be quite easy to accomplish. However, it proved to be a bit trickier than I anticipated. The default convention for table names in EF Core (at the time of writing, version 6) is as follows: If the name of the table is set explicitly in the configuration for a given entity, use that name. If the entity is exposed as a DbSet collection, use the name of the DbSet property. If none of the above, use the entity name.Based on this, we can achieve our desired convention by simply excluding the second rule. So, if the name is set explicitly, use the provided name; otherwise, use the entity name. The ModelBuilder in EF Core exposes an IMutableEntityType collection, which describes all entities included in the model. Additionally, the IMutableEntityType conveniently exposes two methods: GetTableName() and GetDefaultTableName(). At first glance, one might think we can iterate through the entity types and, if these methods return different values, set the entity name. However, GetDefaultTableName returns the entity name or, if a DbSet is defined, its property name. The issue arises when the user sets an explicit table name that is equal to the DbSet property name. In that case, we’ll mistakenly set the entity name and ignore the explicit configuration.Having said that, the final implementation is as follows:protected override void OnModelCreating(ModelBuilder modelBuilder){ // Call the method at the end, after all other configurations modelBuilder.ConfigureTableNames(this);}public static class ModelBuilderExtensions{ public static void ConfigureTableNames(this ModelBuilder modelBuilder, DbContext dbContext) { var dbSetNames = dbContext.GetType().GetProperties() .Where(x =&gt; x.PropertyType.IsGenericType &amp;&amp; typeof(DbSet&lt;&gt;).IsAssignableFrom(x.PropertyType.GetGenericTypeDefinition())) .Select(x =&gt; x.Name) .ToList(); foreach (var entityType in modelBuilder.Model.GetEntityTypes()) { if (!entityType.IsOwned()) { var tableName = entityType.GetTableName(); var defaultTableName = entityType.GetDefaultTableName(); if (tableName is null || defaultTableName is null) continue; if (tableName.Equals(defaultTableName)) continue; if (dbSetNames.Find(x =&gt; x.Equals(tableName)) is not null) { entityType.SetTableName(entityType.DisplayName()); } } } }}I hope you found this article useful. Happy coding!" }, { "title": "Default item for a given entity's collection", "url": "/posts/default-item-for-entity-collections/", "categories": "Software Development", "tags": "dotnetcore, design patterns, software architecture", "date": "2021-05-17 18:00:00 +0200", "snippet": "In this post, I’ll elaborate on the case when you need to mark an item as default for a given entity’s collection. Let’s assume we have Customer and Address entities, and each customer may have mor...", "content": "In this post, I’ll elaborate on the case when you need to mark an item as default for a given entity’s collection. Let’s assume we have Customer and Address entities, and each customer may have more than one address. For the sake of simplicity, we’ll work with the following simple model.public class Customer{ public int Id { get; set; } public List&lt;Address&gt; Addresses { get; set; }}public class Address{ public int Id { get; set; } public string Street { get; set; } public string City { get; set; }}Now, we have a requirement to set a default address for the given customer. I’ll provide few approaches here, but in all of them, we’ll be utilizing the concept of the aggregates. The customer is an aggregate root in this case, and it will be its responsibility to ensure the consistency of the aggregate as a whole. So, the customer will expose a behavior for manipulating its addresses.Solution 1The first approach is the simplest and most straightforward one. We’ll define IsDefault boolean property in the Address entity, and the Customer will set the flag accordingly.public class Address{ public int Id { get; private set; } public string Street { get; private set; } public string City { get; private set; } public bool IsDefault { get; set; } = false; public int CustomerId { get; private set; } public Customer Customer { get; private set; } private Address() { } public Address(string street, string city) { Update(street, city); } public void Update(string street, string city) { if (string.IsNullOrEmpty(street)) throw new ArgumentException(nameof(street)); if (string.IsNullOrEmpty(city)) throw new ArgumentException(nameof(city)); this.Street = street; this.City = city; }}public class Customer{ public int Id { get; private set; } private readonly List&lt;Address&gt; _addresses = new List&lt;Address&gt;(); public IEnumerable&lt;Address&gt; Addresses =&gt; _addresses.AsEnumerable(); public void AddAddress(string street, string city, bool isDefault = false) { var address = new Address(street, city); _addresses.Add(address); if (isDefault) { SetAddressAsDefault(address); } } public void UpdateAddress(int id, string street, string city, bool isDefault = false) { var address = GetAddress(id); address.Update(street, city); if (isDefault) { SetAddressAsDefault(address); } } public void DeleteAddress(int id) { var address = GetAddress(id); _addresses.Remove(address); // Depending on the requirements, if the default address is deleted, the first next one can be assigned as default. } public void SetAddressAsDefault(Address address) { _ = address ?? throw new ArgumentNullException(nameof(address)); _addresses.ForEach(p =&gt; p.IsDefault = false); address.IsDefault = true; } private Address GetAddress(int id) { var address = _addresses.Find(x =&gt; x.Id == id); _ = address ?? throw new KeyNotFoundException($\"The address with id: {id} is not found!\"); return address; }}As you may assume, there are few issues with this approach. We encapsulated the CRUD actions and we’re handling the flag accordingly, but the IsDefault has a public setter. There is no guarantee that this state won’t be mutated outside of the aggregate. Even more importantly, should the address be aware and hold information whether is default or not? It seems wrong conceptually.Solution 2It might be better if the knowledge of which address is the default one is kept in the customer entity. Addresses will be fully agnostic to this information. If following this logic, it might be appealing to define this relation as follows.public class Customer{ public int Id { get; private set; } private readonly List&lt;Address&gt; _addresses = new List&lt;Address&gt;(); public IEnumerable&lt;Address&gt; Addresses =&gt; _addresses.AsEnumerable(); public Address DefaultAddress { get; set; }}This might seem nice, but it’s hard and inconvenient to persist this state. There is already 1:n relationship, and by defining an additional 1:1 relationship, we might end up in a circular dependency. We’ll be forced to introduce an additional foreign key in the Address entity, which is not quite elegant solution.Instead, we can keep only the id of the default address, and define a calculated property that provides the correct address object.public class Customer{ public int Id { get; private set; } private readonly List&lt;Address&gt; _addresses = new List&lt;Address&gt;(); public IEnumerable&lt;Address&gt; Addresses =&gt; _addresses.AsEnumerable(); // This is not a FK. public int? DefaultAddressID { get; private set; } public Address DefaultAddress =&gt; Addresses.FirstOrDefault(x =&gt; x.Id == DefaultAddressID); public void SetAddressAsDefault(Address address) { _ = address ?? throw new ArgumentNullException(nameof(address)); DefaultAddressID = address.Id; } // The rest of the code is same and excluded for clarity.In this case, we don’t have a foreign key to ensure the relationship integrity, but as long as we’re not doing manual changes to the DB directly (and we should not) then it’s an acceptable approach.Solution 3The last option introduces data duplication, but it’s a quite appealing approach. We can keep the default address as an owned type in our entity. It means we’ll have additional columns in our Customer table for persisting the default address. At first glance, it may seem weird, but it offers few advantages In cases where you don’t need all addresses, you won’t have a JOIN operation If you usually need the address when retrieving the customer, it’s handy to have it as an owned type.This will be the implementation, where Address is a value object and CustomerAddress is the related entity.public class Address : ValueObject{ public string Street { get; private set; } public string City { get; private set; } public static Address Empty =&gt; new Address(); private Address() { } public Address(string street, string city) { if (string.IsNullOrEmpty(street)) throw new ArgumentException(nameof(street)); if (string.IsNullOrEmpty(city)) throw new ArgumentException(nameof(city)); this.Street = street; this.City = city; } protected override IEnumerable&lt;object&gt; GetAtomicValues() { yield return Street; yield return City; }}public class CustomerAddress{ public int Id { get; private set; } public Address Details { get; private set; } public int CustomerId { get; private set; } public Customer Customer { get; private set; } private CustomerAddress() { } public CustomerAddress(Address details) { Update(details); } public void Update(Address details) { _ = details ?? throw new ArgumentNullException(nameof(details)); if (Details is null || !Details.Equals(details)) { Details = details; } }}public class Customer{ public int Id { get; private set; } private readonly List&lt;CustomerAddress&gt; _addresses = new List&lt;CustomerAddress&gt;(); public IEnumerable&lt;CustomerAddress&gt; Addresses =&gt; _addresses.AsEnumerable(); // This is owned type, not a navigation. public Address DefaultAddress { get; private set; } = Address.Empty; public void AddAddress(Address address, bool isDefault = false) { _ = address ?? throw new ArgumentNullException(nameof(address)); _addresses.Add(new CustomerAddress(address)); if (isDefault) { SetAddressAsDefault(address); } } public void UpdateAddress(int id, Address address, bool isDefault = false) { _ = address ?? throw new ArgumentNullException(nameof(address)); var customerAddress = GetAddress(id); customerAddress.Update(address); if (isDefault) { SetAddressAsDefault(address); } } public void DeleteAddress(int id) { var customerAddress = GetAddress(id); _addresses.Remove(customerAddress); if (DefaultAddress.Equals(customerAddress.Details)) { DefaultAddress = Address.Empty; } // Depending on the requirements, if the default address is deleted, the first next one can be assigned as default. } public void SetAddressAsDefault(Address address) { _ = address ?? throw new ArgumentNullException(nameof(address)); if (!DefaultAddress.Equals(address)) { DefaultAddress = address; } } private CustomerAddress GetAddress(int id) { var customerAddress = _addresses.Find(x =&gt; x.Id == id); _ = customerAddress ?? throw new KeyNotFoundException($\"The address with id: {id} is not found!\"); return customerAddress; }}" }, { "title": "How to remove a Microsoft work/school account?", "url": "/posts/how-to-remove-microsoft-work-school-account/", "categories": "Blogging, Tutorial", "tags": "microsoft account, azure, office365, outlook", "date": "2021-03-28 18:00:00 +0200", "snippet": "I had a strange experience with my Microsoft account a couple of days ago. My personal Microsoft account got entangled with the newly created Azure AD (work/school) account, and I no longer could u...", "content": "I had a strange experience with my Microsoft account a couple of days ago. My personal Microsoft account got entangled with the newly created Azure AD (work/school) account, and I no longer could use some services (e.g. Outlook). Let me provide some background here and elaborate on it in more detail. I have purchased my domains on GoDaddy years ago, including the premium hosting package. I was managing the mail server by myself, the routing between the domain names (I had two), and all the rest of the details. Some time ago I got sick of all the management tasks, and I purchased a subscription to Office 365 Home provided by Microsoft. As part of the premium options, they offer the ability to associate a custom domain name to your “subscription”, so all the members sharing the package can get an alias mail with the custom domain name. So, let’s summarize, using example person “John Smith” and “smith.com” domain I own the smith.com domain I own Office 365 Home subscription I associated my Office 365 account with the smith.com domain On top of my personal account john@outlook.com, now I got alias mail john@smith.com I have defined john@smith.com as my primary mail/alias. All my devices and services use this Microsoft account, including Windows 10. I use this account for different cloud services provided by Microsoft, including “DevOps”, azure services, etc.This works great and I’ve been quite happy with this setup. But, there are few caveats that you have to consider. Just recently, one of my clients shared few “DevOps” projects with me, which were hosted on azure (dev.azure.com). Since they have been using the Azure AD infrastructure (they might have added the user explicitly or shared directly with john@smith.com), once I accepted the invitation the “entanglement” began. In this scenario, Microsoft just ignores that the domain smith.com is associated with a personal Microsoft account, and automatically creates an AD organization smith.com. Once the directory is created for the base domain, they additionally added the john@smith.com user to the client’s organization/directory as a member (hence completed the sharing part). Now, as you can assume, the smith.com domain exists as an AD organization; and also as a custom domain, part of a personal Microsoft subscription. This setup led to the following oddities Whenever I log in on online Microsoft services (outlook, azure, etc) with john@smith.com, I’m always presented with a choice (a selection) whether I want to login with my personal account or work/school account. I didn’t sign for this, but OK, I can live with it. Some other services, the Outlook (desktop app) in particular, no longer were able to connect to the server. In these cases, the AD account has precedence, and since no services or subscriptions are defined for that account, the applications couldn’t connect. Simply, my mail server is not managed there. Initially, I thought perhaps some local misconfiguration happened, so I tried the following: Removed my account from the Outlook app, and tried to re-add it (which of course I couldn’t) Re-created my mail profile. Tried all kinds of related flags in the registry, autodiscovery options, disabled WAM, etc. Reinstalled the Office suite. None of this worked. To be sure, I tried to connect through a totally different PC, and still, the Outlook client couldn’t add the account (not even as an IMAP account). This assured me that no local configuration matters here. The decision for the authentication/authorization authority is done remotely, and in this case, the AD account always will have a priority.I succeeded to fix the issue. So, here are the steps you have to follow to mitigate this situation. Leave any organization that your newly created Azure AD account is a member, as part of any “sharing” process. Claim ownership of your domain name and get admin privileges. Create a new admin user for your AD organization. Delete your user john@smith.com and your domain smith.com from AD. Delete the user permanently.Leaving all third-party organizationsThe first step will be to remove any association of your account with third-party organizations. Log in to portal.azure.com or login.microsoft.com with your AD account. Simply once you enter your username john@smith.com choose the school/work option. Click on the user icon, and choose “View account”. On the navigation menu on the left, select the “Organizations” menu. Here you’ll find a list of all organizations that are associated with your user. Click on “Leave organization” for any third-party organizations that are listed here. You won’t be able to leave your home organization smith.com, and that’s OK for now. Communicate the owners of those third-party organizations, and kindly ask them to remove any sharing or any other setup related to john@smith.com account. We want to play safe, and be sure there are not any relations left.This will complete this initial step, and once done, you are ready to proceed with the rest of the actions.Claiming admin privileges for your domainEven though Microsoft automatically created the smith.com AD organization, your user john@smith.com has no privileges in this directory. They can’t be sure if you really own this domain name, and to claim the ownership, you’ll have to go through few verification steps. Log in to portal.azure.com or login.microsoft.com with your AD account. Simply once you enter your username john@smith.com choose the school/work option. Click on the user icon, and choose “View account”. Click on the options icon on the top left corner of the navigation menu, and choose “Admin”. Since your account is still not the admin of the directory, a new page for claiming ownership will be presented. It should state something like “Ready to become the Admin for school.com?”. The wizard consists of just a couple of steps. and the provided descriptions are quite explanatory. All you will have to do, is add a TXT record in your DNS configuration with a value that is provided in this step. If you’re an owner of the domain name, this is quite a straightforward task. Most often your DNS management will be part of your hosting platform, so your hosting provider will have some tool/page for managing the DNS records. If you’re managing the DNS separately (e.g. Cloudflare), then you will complete the configuration there. Once you have added the TXT record (wait 5-10 minutes), finish the steps in the wizard. If verification succeeds, your user john@smith.com will gain admin privileges.Create a new Admin account.Now, that you are an admin of the organization, the “Admin” page will contain a lot more options. Open the “Admin” center. If you have closed your browser, just follow the previous steps. Log in, click “View Account”, and choose “Admin” from the navigations. Click on the “Users” menu, and then on “Active Users”. The only user listed will be john@smith.com. Click on “Add User”. Populate the basic information for the user, “First Name”, “Last Name”, “Display Name”. For the username choose anything you like, it might be simply “admin”. Now, the most important part, on the “Domains” dropdown section, one additional option will be available smith.onmicrosoft.com. It’s crucial that you select this option, so we can free up our domain name. Uncheck the “Automatically create a password” and enter a password for this account. Uncheck the “Require this user to change their password when they first sign-in” to simplify the process. Click “Next”. On the second step “Product Licenses”, just choose your location, and be sure that the “Right Management Adhoc” license is selected. Since you haven’t used this account for anything this would be the only license listed here. Click “Next”. On the “Optional Settings” page, it’s important to assign admin privileges to this user. Expand the “Roles” section, select “Admin Center Access”, and check the “Global Administrator” role. Click “Next”. Finish adding the user. Logout from john@smith.com.Deleting your user and your domain from ADNow that we have an additional admin user (which is defined on the smith.onmicrosoft.com domain), we can delete our user and our domain from the organization. Log in to the same page, this time with admin@smith.onmicrosoft.com user. Get to the “Users” page, select your user john@smith.com, and choose “Delete a user”. On the navigation menu, select the “Show All” option. On the expanded menu, choose “Setup” and then “Domains”. On this page two domains will be listed smith.com and smith.onmicrosoft.com. Select the smith.com domain and delete it. Logout from everywhere.Deleting your user permanentlyWhen you delete a user, Microsoft keeps the record for additional 30 days, just in case you change your mind and want to recover it. In our case, we’d like to delete it permanently. Log in to portal.azure.com with admin@smith.onmicrosoft.com This time instead of choosing “View account”, choose “Azure Active Directory”. Most probably it will be listed on the front page under the “Azure services” section. You can find it on the left navigation menu as well. Choose the “Users” menu. Choose the “Deleted users” menu. Select your deleted user john@smith.com and delete it permanently.At this stage, you have completed all required actions. You should be a little patient and give it some time (a couple of days). In my case, after 1 day I was able to successfully add my mail account through the Outlook client. I hope this article will save you some time, and it will be of help if you find yourself in the same situation.NOTE: I avoided adding screenshots in this article since the UI is changing often and might mislead you. I tried to be quite descriptive in the provided steps, and hopefully, they are easy to follow. Cheers!" }, { "title": "What is the proper usage of domain events?", "url": "/posts/what-is-the-proper-usage-of-domain-events/", "categories": "Software Development", "tags": "ddd, design patterns, dotnetcore", "date": "2021-03-11 17:00:00 +0100", "snippet": "Events are a powerful concept and they’re quite a useful mechanism in various scenarios. They enable us to notify other parties of our internal activities, simply put, broadcasting our actions. We ...", "content": "Events are a powerful concept and they’re quite a useful mechanism in various scenarios. They enable us to notify other parties of our internal activities, simply put, broadcasting our actions. We all have used events in one form or another. They are very common in desktop application development, where the “framework” fires events on various user interactions with UI, and we respond to these events by doing something useful. Anyhow, that’s not the only place where events can be utilized. If you are fond of Domain-Driven Design (DDD), it’s common to implement domain events as a communication mechanism between the aggregates, or just passing some information to the outer “world”. It offers some form of loose coupling of different modules, which usually act quite independently within their own boundaries. But, it’s very easy and it’s very common to misuse the concept of events.Let’s examine the following sample, and try to figure out if we’re doing the correct thing.public class OrderItem{ public int Id { get; } public string Name { get; } public decimal Quantity { get; private set; } public decimal Price { get; }\t\tpublic int OrderId { get; } public event EventHandler QuantityChanged; public OrderItem(string name, decimal quantity, decimal price) { if (string.IsNullOrEmpty(name)) throw new ArgumentNullException(nameof(name)); this.Name = name; this.Quantity = quantity; this.Price = price; } public void UpdateQuantity(decimal quantity) { this.Quantity = quantity; this.QuantityChanged?.Invoke(this, new EventArgs()); }}public class Order{ public int Id { get; } public decimal GrandTotal { get; private set; } private readonly List&lt;OrderItem&gt; _orderItems = new List&lt;OrderItem&gt;(); public IEnumerable&lt;OrderItem&gt; OrderItems =&gt; _orderItems.AsEnumerable(); public OrderItem AddItem(OrderItem orderItem) { _ = orderItem ?? throw new ArgumentNullException(nameof(orderItem)); _orderItems.Add(orderItem); CalculateGrandTotal(); return orderItem; } public void DeleteOrderItem(int orderItemId) { var orderItem = OrderItems.FirstOrDefault(x =&gt; x.Id == orderItemId); _ = orderItem ?? throw new KeyNotFoundException($\"The order item with Id: {orderItemId} is not found!\"); _orderItems.Remove(orderItem); CalculateGrandTotal(); } public void Order_QuantityChanged(object sender, EventArgs e) { CalculateGrandTotal(); } private void CalculateGrandTotal() { this.GrandTotal = this.OrderItems.Sum(x =&gt; x.Price); }}In this case, whenever the OrderItem is changed, it raises an appropriate event. On the other hand, the Order, the aggregate root in this sample, listens to this event and updates the GrandTotal information on each change of its items. For simplicity, we’ll assume the subscribing action happens in some service (e.g., in OrderService). This implementation is fairly common, and I see it all the time. I have used this pattern very often in the past too. So, what’s wrong here, if anything?If we try to generalize, we might identify 3 types of communication mechanisms or types of interactions in our system Queries - I want to know something. Please provide me this information. Commands - I want something to happen. Please do this/that. Events - I’m just doing my business. I’m talking loudly, but I really don’t care if anyone is listening.In our sample, the CalculateGrandTotal is not a trivial or an optional operation. On contrary, it’s a crucial action that must happen to preserve the consistency of the aggregate. If we miss this action we’ll certainly end up with corrupted/incorrect data, the GrandTotal will contain an invalid value. Having this in mind, the OrderItem is not raising the event carelessly, but with a specific intention, it expects something specific to happen as a result. If we can paraphrase, this is what OrderItem is shouting“I want to notify the world that I have been changed. But, hey Order, it’s important you listen to this, and it’s important that you do that action. Hey Order… Order…”It’s quite clear that we’re doing something wrong here. We clearly want something to happen as a side effect of our actions. This no longer should be an event, but it’s a command. Instead of notifying someone and hoping that the necessary action will happen, we can simply demand that action. Let’s modify the sample according to this.public class OrderItem{ public int Id { get; } public string Name { get; } public decimal Quantity { get; private set; } public decimal Price { get; } public int OrderId { get; } public Order Order { get; } public OrderItem(string name, decimal quantity, decimal price) { if (string.IsNullOrEmpty(name)) throw new ArgumentNullException(nameof(name)); this.Name = name; this.Quantity = quantity; this.Price = price; } public void UpdateQuantity(decimal quantity) { this.Quantity = quantity; this.Order?.CalculateGrandTotal(); }}public class Order{ public int Id { get; } public decimal GrandTotal { get; private set; } private readonly List&lt;OrderItem&gt; _orderItems = new List&lt;OrderItem&gt;(); public IEnumerable&lt;OrderItem&gt; OrderItems =&gt; _orderItems.AsEnumerable(); public OrderItem AddItem(OrderItem orderItem) { _ = orderItem ?? throw new ArgumentNullException(nameof(orderItem)); _orderItems.Add(orderItem); CalculateGrandTotal(); return orderItem; } public void DeleteOrderItem(int orderItemId) { var orderItem = OrderItems.FirstOrDefault(x =&gt; x.Id == orderItemId); _ = orderItem ?? throw new KeyNotFoundException($\"The order item with Id: {orderItemId} is not found!\"); _orderItems.Remove(orderItem); CalculateGrandTotal(); } public void CalculateGrandTotal() { this.GrandTotal = this.OrderItems.Sum(x =&gt; x.Price); }}Now, we still keep the calculation logic in Order, but its items can demand this action in a non-ambiguous way. The intent is communicated more clearly.The argument against this approach, usually is “you have to remember to call the method”. But, in no form that’s any different from raising the event, you still have to remember to raise the event. We can summarize as follows: Command case - I should remember to call this function here Event case - I should remember to raise the event here. Also, I will need you specifically to subscribe to it.It’s obvious (for me at least), what’s the correct thing to do here. The other argument is that we shouldn’t have a navigation back to the aggregate. That no longer is an issue, and it’s quite safe to use it as it is. But, you can always just accept the aggregates’ method as an Action parameter, and achieve the same." }, { "title": "Immutable entities and value objects in EF Core!", "url": "/posts/immutable-entities-in-ef-core/", "categories": "Software Development", "tags": "EFCore, dotnetcore", "date": "2021-03-10 17:00:00 +0100", "snippet": "You can find the code and the full sample here.Immutability is a trendy topic nowadays. The immutable constructs can be handy and can offer many benefits in different scenarios. They can significan...", "content": "You can find the code and the full sample here.Immutability is a trendy topic nowadays. The immutable constructs can be handy and can offer many benefits in different scenarios. They can significantly improve your domain model by capturing and mimicking various business processes more accurately. We won’t dive into all the pros and cons of adopting the immutability, which will require a separate article. We’ll only demonstrate how to implement and persist them in EF Core once you have decided to utilize such constructs in your design.Let’s start with a sample model as shown belowpublic class Order{ public int Id { get; set; } public string OrderNo { get; set; } public DateTime Date { get; set; } public string CustomerFirstName { get; set; } public string CustomerLastName { get; set; } public string CustomerEmail { get; set; } public string Street { get; set; } public string City { get; set; } public string PostalCode { get; set; } public string Country { get; set; } public decimal GrandTotal { get; set; } public List&lt;OrderItem&gt; OrderItems = new List&lt;OrderItem&gt;();}public class OrderItem{ public int Id { get; set; } public string Name { get; set; } public decimal Price { get; set; }}In this scenario, we have defined Order and OrderItem models. For now, they’re fully anemic models, with no behavior, and no encapsulation applied. Simply said, they’re just data objects. In the next sections, we’ll improve the model and define the necessary configurations for EF Core.Designing the domain modelWhile designing the entities and the domain model in general, it is wise to step back, disconnect yourself from the code and think of the actual business domain in the real world. In our case, first and foremost, is there any concept in the real-world which corresponds to an “empty order”? Imagine you’re trying to buy a product, but you oppose sharing any information about the recipient, name, address to be shipped, or any other info. That doesn’t make much sense right? The “order” becomes a “thing” once it contains a certain piece of information, otherwise, it’s just a meaningless word. Having that in mind, then the question is why would we allow an empty Order object in our application? Obviously, our object should utilize a constructor, and accept all required information as parameters. Once instantiated, we want to be sure that the object is not in a corrupted state.The next step would be to go through all the properties and try to make sense of what they represent and what changes we can introduce to improve the model Id - This is an identifier of the entity/record. In our case, it’s not a business concept but just required technical information. Once the entity is assigned an identifier, there is no reason for the Id to change. OrderNo - This represents the number of the document/order. This should be a unique identifier, composed of some given business rules. Once the document gets its number, no longer we should be able to change it. Date - It is the date of the document. In this example, we’ll require the exact date and time to be set automatically by the system itself, not as an input by the user. Additionally, instead of directly setting the date, we’ll accept a given service that can provide the required information. This will enable us to mock the service and write better unit tests. Customer information - We’d like to group all customer information in a separate construct, and we’ll use a value object for that purpose. The customer information should be provided while we create the order. No changes should be allowed, otherwise, that would represent a completely different order. Address information - We might group the address information in a separate value object too. It is required information to create an order, but the customer can change their mind and update the desired shipping address. GrandTotal - It’s the sum of the prices of the items in the order. The amount should be automatically updated as we add or delete items from the order. OrderItems - It’s a list of items in the current order. In this example, once we add an item, we can no longer change it, but only delete it. We also want to encapsulate the actions of adding/deleting items within the Order entity.Now that we defined our model, we can refactor the Order and OrderItem as follows.public class Customer : ValueObject{ public string FirstName { get; } public string LastName { get; } public string Email { get; } public Customer(string firstName, string lastName, string email) { if (string.IsNullOrEmpty(firstName)) throw new ArgumentNullException(nameof(firstName)); if (string.IsNullOrEmpty(lastName)) throw new ArgumentNullException(nameof(lastName)); this.FirstName = firstName; this.LastName = lastName; this.Email = email; } protected override IEnumerable&lt;object&gt; GetAtomicValues() { yield return this.FirstName; yield return this.LastName; yield return this.Email; }}public class Address : ValueObject{ public string Street { get; } public string City { get; } public string PostalCode { get; } public string Country { get; } public Address(string street, string city, string postalCode, string country) { if (string.IsNullOrEmpty(street)) throw new ArgumentNullException(nameof(street)); if (string.IsNullOrEmpty(city)) throw new ArgumentNullException(nameof(city)); if (string.IsNullOrEmpty(postalCode)) throw new ArgumentNullException(nameof(postalCode)); if (string.IsNullOrEmpty(country)) throw new ArgumentNullException(nameof(country)); this.Street = street; this.City = city; this.PostalCode = postalCode; this.Country = country; } protected override IEnumerable&lt;object&gt; GetAtomicValues() { yield return this.Street; yield return this.City; yield return this.PostalCode; yield return this.Country; }}public class OrderItem{ public int Id { get; } public string Name { get; } public decimal Price { get; } public OrderItem(string name, decimal price) { if (string.IsNullOrEmpty(name)) throw new ArgumentNullException(nameof(name)); this.Name = name; this.Price = price; }}public class Order{ public int Id { get; } public string OrderNo { get; } public DateTime Date { get; } public Customer Customer { get; } public Address Address { get; private set; } public decimal GrandTotal { get; private set; } private readonly List&lt;OrderItem&gt; _orderItems = new List&lt;OrderItem&gt;(); public IEnumerable&lt;OrderItem&gt; OrderItems =&gt; _orderItems.AsEnumerable(); public Order(IDateTime dateTimeService, string orderNo, Customer customer, Address address) { _ = dateTimeService ?? throw new ArgumentNullException(nameof(dateTimeService)); _ = customer ?? throw new ArgumentNullException(nameof(customer)); if (string.IsNullOrEmpty(orderNo)) throw new ArgumentNullException(nameof(orderNo)); //this.Id = if we decide to use DB generated value no action required. this.OrderNo = orderNo; this.Date = dateTimeService.Now; this.Customer = customer; UpdateAddress(address); } public void UpdateAddress(Address address) { _ = address ?? throw new ArgumentNullException(nameof(address)); if (this.Address is null || !this.Address.Equals(address)) { this.Address = address; } } public OrderItem AddItem(OrderItem orderItem) { _ = orderItem ?? throw new ArgumentNullException(nameof(orderItem)); _orderItems.Add(orderItem); CalculateGrandTotal(); return orderItem; } public void DeleteOrderItem(int orderItemId) { var orderItem = OrderItems.FirstOrDefault(x =&gt; x.Id == orderItemId); _ = orderItem ?? throw new KeyNotFoundException($\"The order item with Id: {orderItemId} is not found!\"); _orderItems.Remove(orderItem); CalculateGrandTotal(); } private void CalculateGrandTotal() { this.GrandTotal = this.OrderItems.Sum(x =&gt; x.Price); }}We captured all the rules and modeled the entities accordingly. As you can notice, some of the properties do not expose even private setters. There is no reason for those properties to change within or out of the boundaries of the class. It’s important to emphasize that the compiler will generate readonly shadow backing fields for these properties, so they are purely immutable. We also defined the OrderItem, Customer, and Address objects as fully immutable.EF Core configurationNow that we have properly designed our domain model, the question is how to persist them in this form. As the encapsulation is concerned, EntityFramework Core has great support. If configured, it can work directly with private backing fields allowing better control of what we publicly expose. We can even define automatic properties, and as long they expose at least private setters, EF Core can automatically work with the private shadow fields. This usage is quite common and works quite well. But, it’s a less known fact that EF Core can do much better and utilize even constructors while creating instances of the given entities. By leveraging this feature, we can design purely immutable entities.Before defining the configuration, I’d like to emphasize few topics which can save you in the troubleshooting process. EF by default excludes/ignores all the properties with getters only. It assumes they are calculated properties. For those properties, we should explicitly configure EF to include them in the model. If we have included readonly properties in the model, EF will require a constructor (it may be marked as private) with matching parameters for these properties. You must have a constructor which accepts parameters only for these properties, not including the mutable ones. The constructor parameters should be named exactly as the properties. Excluding the first character, the naming is case-sensitive. For a given FirstName property, you may define the parameter as FirstName or firstName, but not as firstname. This is a common mistake. If you have defined immutable owned types (e.g., value objects) as part of the entity, you might be compelled to add them as constructor parameters. Doing so will be an incorrect configuration, and you’ll face runtime exceptions. EF Core creates instances of these objects separately and has a different internal mechanism of mapping these objects to the entity.Finally, the entities will have the following form (Customer and Address value objects remain unchanged):public class OrderItem{ public int Id { get; } public string Name { get; } public decimal Price { get; } public int OrderId { get; } private OrderItem(int id, string name, decimal price, int orderId) { this.Id = id; this.Name = name; this.Price = price; this.OrderId = orderId; } public OrderItem(string name, decimal price) { if (string.IsNullOrEmpty(name)) throw new ArgumentNullException(nameof(name)); this.Name = name; this.Price = price; }}public class Order{ public int Id { get; } public string OrderNo { get; } public DateTime Date { get; } public Customer Customer { get; } public Address Address { get; private set; } public decimal GrandTotal { get; private set; } private readonly List&lt;OrderItem&gt; _orderItems = new List&lt;OrderItem&gt;(); public IEnumerable&lt;OrderItem&gt; OrderItems =&gt; _orderItems.AsEnumerable(); // Just to demostrate calculated properties. public string GrandTotalNormalized =&gt; this.GrandTotal.ToString(\"n2\"); private Order(int id, string orderNo, DateTime date) { this.Id = id; this.OrderNo = orderNo; this.Date = date; } public Order(IDateTime dateTimeService, string orderNo, Customer customer, Address address) { _ = dateTimeService ?? throw new ArgumentNullException(nameof(dateTimeService)); _ = customer ?? throw new ArgumentNullException(nameof(customer)); if (string.IsNullOrEmpty(orderNo)) throw new ArgumentNullException(nameof(orderNo)); //this.Id = if we decide to use DB generated value no action required. this.OrderNo = orderNo; this.Date = dateTimeService.Now; this.Customer = customer; UpdateAddress(address); } /// It's important not to update the Address if there are no changes. /// If updated, since it's a new object, EF tracker will mark it as New. /// This can have implications on \"Auditing\" logic, if you have implemented one. public void UpdateAddress(Address address) { _ = address ?? throw new ArgumentNullException(nameof(address)); if (this.Address is null || !this.Address.Equals(address)) { this.Address = address; } } public OrderItem AddItem(OrderItem orderItem) { _ = orderItem ?? throw new ArgumentNullException(nameof(orderItem)); _orderItems.Add(orderItem); CalculateGrandTotal(); return orderItem; } public void DeleteOrderItem(int orderItemId) { var orderItem = OrderItems.FirstOrDefault(x =&gt; x.Id == orderItemId); _ = orderItem ?? throw new KeyNotFoundException($\"The order item with Id: {orderItemId} is not found!\"); _orderItems.Remove(orderItem); CalculateGrandTotal(); } private void CalculateGrandTotal() { this.GrandTotal = this.OrderItems.Sum(x =&gt; x.Price); }}The required EF configuration for these entities is as followingpublic class OrderItemConfiguration : IEntityTypeConfiguration&lt;OrderItem&gt;{ public void Configure(EntityTypeBuilder&lt;OrderItem&gt; builder) { builder.ToTable(nameof(OrderItem)); builder.Property(x =&gt; x.Name).IsRequired().HasMaxLength(100); builder.Property(x =&gt; x.Price).HasPrecision(18, 2); // Even though we're not configuring anything specific for OrderId property, // it's crucial to write the following line, and let the EF know that we want to include this property. // EF by default, excludes all properties with getters only. It assumes they are calculated properties. builder.Property(x =&gt; x.OrderId); builder.HasKey(x =&gt; x.Id); }}public class OrderConfiguration : IEntityTypeConfiguration&lt;Order&gt;{ public void Configure(EntityTypeBuilder&lt;Order&gt; builder) { builder.ToTable(nameof(Order)); // Even though we're not configuring anything specific for Date property, // it's crucial to write the following line, and let the EF know that we want to include this property. // EF by default, excludes all properties with getters only. It assumes they are calculated properties. builder.Property(x =&gt; x.Date); builder.Property(x =&gt; x.OrderNo).IsRequired().HasMaxLength(10); builder.Property(x =&gt; x.GrandTotal).HasPrecision(18, 2); builder.OwnsOne(x =&gt; x.Customer, o =&gt; { o.WithOwner(); o.Property(x =&gt; x.FirstName).IsRequired().HasMaxLength(100); o.Property(x =&gt; x.LastName).IsRequired().HasMaxLength(100); o.Property(x =&gt; x.Email).HasMaxLength(100); }); builder.OwnsOne(x =&gt; x.Address, o =&gt; { o.WithOwner(); o.Property(x =&gt; x.Street).IsRequired().HasMaxLength(250); o.Property(x =&gt; x.City).IsRequired().HasMaxLength(100); o.Property(x =&gt; x.PostalCode).IsRequired().HasMaxLength(10); o.Property(x =&gt; x.Country).IsRequired().HasMaxLength(100); }); builder.Metadata.FindNavigation(nameof(Order.OrderItems)) .SetPropertyAccessMode(PropertyAccessMode.Field); builder.HasKey(x =&gt; x.Id); }}Once you have this in place, EF Core will be able to do its magic and your application with run smoothly.Hopefully, this article will be of help to you, and you’ll be able to improve your domain model and embrace immutability whenever is required.You can find the full sample here." }, { "title": "Alternative caching implementations and cache invalidation!", "url": "/posts/alternative-caching-implementations-and-cache-invalidation/", "categories": "Software Development", "tags": "cache, dotnetcore, design patterns, software architecture", "date": "2020-12-09 17:00:00 +0100", "snippet": "Today I’ll be talking about caching and various ways to implement the same. We’ll also show how to invalidate the cache on demand if using Entity Framework.The caching feature is almost mandatory f...", "content": "Today I’ll be talking about caching and various ways to implement the same. We’ll also show how to invalidate the cache on demand if using Entity Framework.The caching feature is almost mandatory for your applications, especially if you have a lot of relatively read-only data. It can drastically boost up the performance, by reducing the roundtrips to your database or any other persistence infrastructure that you might have.The standard and most common way of implementing caching is by using a decorator pattern (or is it proxy pattern? Everlasting discussion) and invalidating the cache on specific time intervals.OK, let’s imagine a scenario, and try to provide various possible solutions. We have Customer data in our application, and we utilize this information very often. In the case of a desktop application, we might have a sort of drop-down or search controls; and in a web application might be a search field with a drop-down suggestion list. Other than that, the application uses customers heavily in various scenarios. The behavior of several features depends on the customer, and we querying this information quite often.To sum up the requirements: We have customers entity/table in our app. The customer table contains roughly 1.000 records. The records barely change, once per day, or even once per week. But, once the data is changed, it’s crucial we have the new data and work with the most up-to-date information. We require a complete list of customers, for whatever reason. Based on all information above, we surely want to utilize some caching infrastructure and reduce the number of queries to persistence.Solution 1The common approach, as mentioned above, is to use the decorator pattern. Initially, we have a service/repository (or whatever construct) which queries the persistence and gets the data. Then, we create an additional implementation that contains the same actions but wraps/decorates each of them with caching functionality. The rest of the application consumes the cached variant of the service.The customer entitypublic class Customer{ public int Id { get; set; } public string Name { get; set; } public string Address { get; set; } public string Email { get; set; }}public interface ICustomerRepository{ Task&lt;List&lt;Customer&gt;&gt; GetCustomers();}public class CustomerRepository{ private readonly AppDbContext dbContext; public CustomerRepository(AppDbContext dbContext) { this.dbContext = dbContext; } public Task&lt;List&lt;Customer&gt;&gt; GetCustomers() { return dbContext.Customers.ToListAsync(); }}public class CustomerCachedRepository : ICustomerRepository{ private readonly IMemoryCache cache; private readonly CustomerRepository customerRepository; private readonly string cacheKey = nameof(Customer); private readonly TimeSpan cacheDuration = TimeSpan.FromSeconds(1); public CustomerCachedRepository(IMemoryCache cache, CustomerRepository customerRepository) { this.cache = cache; this.customerRepository = customerRepository; } public Task&lt;List&lt;Customer&gt;&gt; GetCustomers() { return cache.GetOrCreateAsync(cacheKey, entry =&gt; { entry.SlidingExpiration = cacheDuration; return customerRepository.GetCustomers(); }); }}We’ll register the services as scoped, as shown below. Note that the cached repository accepts concrete implementation and not the interface, otherwise we’ll end up with a circular dependency.services.AddMemoryCache();services.AddScoped&lt;ICustomerRepository, CustomerCachedRepository&gt;();services.AddScoped&lt;CustomerRepository&gt;();Solution 2The previous solution is quite straightforward and does the job well. The only issue with this approach is the “timer”. We are invalidating the cache on strictly defined time intervals. Choosing the right time interval is not as easy as it seems. Define a short interval (a couple of seconds) and you end with too many queries, define a longer one and you risk working with obsolete data.In our scenario, customers barely change, and yet we end up querying them each second. So, we should define the interval in hours maybe? In that case, when changes occur, we surely will end up corrupting our data.So, let’s try another approach, implement caching manually and invalidate the cache on-demand, on change only. We’ll create a new specific interface and name it ICachedDataService. We’ll modify the CustomerRepository as followingpublic interface ICachedDataService{ Task Reload(AppDbContext dbContext);}public interface ICustomerRepository{ IEnumerable&lt;Customer&gt; Customers { get; }}public class CustomerRepository : ICustomerRepository, ICachedDataService{ private static readonly object _instanceLock = new object(); public IEnumerable&lt;Customer&gt; Customers { get; private set; } public async Task Reload(AppDbContext dbContext) { var customers = await dbContext.Customers.ToListAsync(); lock (_instanceLock) { Customers = customers; } }}No doubt this construct should have a singleton scope. We want a single instance so the list of customers will act as static information. Surely, you can have a static class here, or implement a singleton pattern manually. But, be aware in that case the consumers will be strongly coupled to the implementation, and you might no be able to easily switch it and unit test the consumers properly. My advice, if you already using a DI container, just let it handle the scope for you, and utilize constructor injection in the consumers.services.AddSingleton&lt;ICustomerRepository, CustomerRepository&gt;();services.AddSingleton&lt;ICachedDataService, CustomerRepository&gt;();And, finally, let’s invalidate the cache in the persistence implementation, in our case EntityFramework. Override the SaveChanges as shown belowpublic class AppDbContext : DbContext{ private readonly ICachedDataService cachedDataService; public DbSet&lt;Customer&gt; Customers { get; set; } public AppDbContext(DbContextOptions&lt;AppDbContext&gt; options, ICachedDataService cachedDataService) : base(options) { this.cachedDataService = cachedDataService; } protected override void OnModelCreating(ModelBuilder modelBuilder) { base.OnModelCreating(modelBuilder); modelBuilder.ApplyConfiguration(new CustomerConfiguration()); } public override async Task&lt;int&gt; SaveChangesAsync(CancellationToken cancellationToken = default) { var isCustomerChanged = ChangeTracker.Entries&lt;Customer&gt;().Where(x =&gt; x.IsAddedOrModifiedOrDeleted()).Count() &gt; 0; var response = await base.SaveChangesAsync(cancellationToken); if (isCustomerChanged) { await cachedDataProvider.Reload(this); } return response; }}Note the extension method. If entities have owned types, you may want to check them for changes as well. There are some improvements in the newer versions, but previously, EF was not flagging the entity as modified if any of the owned types have changed.public static class ChangeTrackerExtensions{ public static bool IsAddedOrModifiedOrDeleted(this EntityEntry entry) =&gt; entry.State == EntityState.Deleted || entry.State == EntityState.Added || entry.State == EntityState.Modified || entry.References.Any(r =&gt; r.TargetEntry != null &amp;&amp; r.TargetEntry.Metadata.IsOwned() &amp;&amp; (r.TargetEntry.State == EntityState.Added || r.TargetEntry.State == EntityState.Modified));}Now, whenever the customer information is changed, we’ll reload the cached data immediately, and the consumers will work with up-to-date customers.Initially, we’ll read/reload the data on application startup. You can do it as shown here.public class Program{ public static async Task Main(string[] args) { var host = CreateHostBuilder(args).Build(); using (var scope = host.Services.CreateScope()) { var services = scope.ServiceProvider; var dbContext = services.GetRequiredService&lt;AppDbContext&gt;(); var cachedDataService = services.GetRequiredService&lt;ICachedDataService&gt;(); await cachedDataService.Reload(dbContext); } host.Run(); } public static IHostBuilder CreateHostBuilder(string[] args) =&gt; Host.CreateDefaultBuilder(args) .ConfigureWebHostDefaults(webBuilder =&gt; { webBuilder.UseStartup&lt;Startup&gt;(); });}Solution 3What if we want several entities cached? Let’s say we want to cache a list of countries too.We’ll start by creating a few additional abstractions and refactor the implementations. Let’s define an interface which will mark the entities that we want to cache.public interface ICacheInfo{ string CacheKey { get; }}public class Customer : ICacheInfo{ public string CacheKey =&gt; nameof(Customer); public int Id { get; set; } public string Name { get; set; } public string Address { get; set; } public string Email { get; set; }}public class Country : ICacheInfo{ public string CacheKey =&gt; nameof(Country); public int Id { get; set; } public string Name { get; set; } public string Code { get; set; }}And few changes in the repository implementationspublic interface ICachedDataService : ICacheInfo{ Task Reload(AppDbContext dbContext);}public interface ICustomerRepository{ IEnumerable&lt;Customer&gt; Customers { get; }}public interface ICountryRepository{ IEnumerable&lt;Country&gt; Countries { get; }}public class CustomerRepository : ICustomerRepository, ICachedDataService{ private static readonly object _instanceLock = new object(); public string CacheKey { get; } = nameof(Customer); public IEnumerable&lt;Customer&gt; Customers { get; private set; } public async Task Reload(AppDbContext dbContext) { var customers = await dbContext.Customers.ToListAsync(); lock (_instanceLock) { Customers = customers; } }}public class CountryRepository : ICountryRepository, ICachedDataService{ private static readonly object _instanceLock = new object(); public string CacheKey { get; } = nameof(Country); public IEnumerable&lt;Country&gt; Countries { get; private set; } public async Task Reload(AppDbContext dbContext) { var countries = await dbContext.Countries.ToListAsync(); lock (_instanceLock) { Countries = countries; } }}Now let’s modify the SaveChanges and the Reload implementations as well.public class AppDbContext : DbContext{ private readonly IEnumerable&lt;ICachedDataService&gt; cachedDataServices; public DbSet&lt;Customer&gt; Customers { get; set; } public DbSet&lt;Country&gt; Countries { get; set; } public AppDbContext(DbContextOptions&lt;AppDbContext&gt; options, IEnumerable&lt;ICachedDataService&gt; cachedDataServices) : base(options) { this.cachedDataServices = cachedDataServices; } protected override void OnModelCreating(ModelBuilder modelBuilder) { base.OnModelCreating(modelBuilder); modelBuilder.ApplyConfiguration(new CustomerConfiguration()); modelBuilder.ApplyConfiguration(new CountryConfiguration()); } public override async Task&lt;int&gt; SaveChangesAsync(CancellationToken cancellationToken = default) { var keys = ChangeTracker.Entries&lt;ICacheInfo&gt;() .Where(x =&gt; x.IsAddedOrModifiedOrDeleted()) .Select(x =&gt; x.Entity.CacheKey) .Distinct() .ToList(); var response = await base.SaveChangesAsync(cancellationToken); foreach (var key in keys) { await cachedDataServices.FirstOrDefault(x =&gt; x.CacheKey.Equals(key))?.Reload(this); } return response; }}In EF Core 5, we can use interceptors or events, so no need to overload the SaveChangesAsync method. But, the above approach will work in almost all versions, and that’s why I’m using it here.Based on the new modified infrastructure, we have to be careful about how we register the services in the DI container. For an instance, we want both ICustomerRepository and ICachedDataService to return the same instance. If we do the following, we’ll get two singleton instances for each interface separately. And, that’s not what we want.services.AddSingleton&lt;ICustomerRepository, CustomerRepository&gt;();services.AddSingleton&lt;ICachedDataService, CustomerRepository&gt;();In order to mitigate this, we can simply create the instance manually and provide it to the DI container.var customerRepository = new CustomerRepository();services.AddSingleton&lt;ICustomerRepository&gt;(customerRepository);services.AddSingleton&lt;ICachedDataService&gt;(customerRepository);var countryRepository = new CountryRepository();services.AddSingleton&lt;ICountryRepository&gt;(countryRepository);services.AddSingleton&lt;ICachedDataService&gt;(countryRepository);Finally, we’ll modify the startup code as wellpublic class Program{ public static async Task Main(string[] args) { var host = CreateHostBuilder(args).Build(); using (var scope = host.Services.CreateScope()) { var services = scope.ServiceProvider; var dbContext = services.GetRequiredService&lt;AppDbContext&gt;(); var cachedDataServices = services.GetRequiredService&lt;IEnumerable&lt;ICachedDataService&gt;&gt;(); foreach (var cachedDataService in cachedDataServices) { await cachedDataService.Reload(dbContext); } } host.Run(); } public static IHostBuilder CreateHostBuilder(string[] args) =&gt; Host.CreateDefaultBuilder(args) .ConfigureWebHostDefaults(webBuilder =&gt; { webBuilder.UseStartup&lt;Startup&gt;(); });}SummaryYou can find the full sample in the following repo here. In the sample, “Cities” are implemented through IMemoryCache and decorator pattern, while the “Customer” and “Countries” with the alternative approach.The standard caching implementation serves us well in various use cases. Using that approach it’s really easy to cache various sets based on the input filters (paginated results, applied criteria, etc).In specific use cases, where we don’t have a large set of data, and we want to hold the whole collection; then the described alternative approaches might be a better solution.Next time, I’ll describe how to improve this infrastructure even further. We’ll create better abstractions to remove some boilerplate code, and will provide the ability to cache various filtered sets of the collection. Practically, on cache invalidation, we’ll invalidate all caches for that particular entity, and then cache various filtered data on the next first request." }, { "title": "Open-Closed Principle and runtime DI configurations!", "url": "/posts/open-close-principle-and-runtime-di-configuration/", "categories": "Software Development", "tags": "dependency injection, dotnetcore, design patterns, software architecture", "date": "2020-12-02 17:00:00 +0100", "snippet": "I just recently published a Nuget package that offers some extensions to the .NET Core’s built-in DI container, practically extensions to IServiceCollection. The extensions provide the ability for ...", "content": "I just recently published a Nuget package that offers some extensions to the .NET Core’s built-in DI container, practically extensions to IServiceCollection. The extensions provide the ability for dynamic/runtime DI configuration, through external config files. You can find the repo here.This is not a new approach at all, and in some other platforms is being heavily used. If anyone used Java with Spring, at some point, you surely have utilized XML based injection. On the other hand in .NET this is actively discouraged, for a good reason. Giving up the compile-time error proofing not always is a bright idea. Of course, there is a difference in how “linking” and “reflection” works in Java and .NET, and that affects the design decisions. But, generally, strongly typed and compile-time configurations are always favored in .NET world.Then, what all this is about? What’s the benefit here, if any?MotivationShortly said, such a design offers the ability to switch implementations on the fly, on runtime, not requiring to re-compile the solution; restarting the application is all you need. But, do we really need that? We already have other mechanisms and patterns on how to provide some form of flexibility in this context. Before going any further, let’s just shortly remind ourselves of Open-Closed Principle, and what that means.As a brief recap, the OCP predicates that we should have constructs that are open to extensions and closed to changes. This means if we need to add a “behavior” to a solution, or even to a class; we should be able to do that without changing the existing constructs. Even more simplified, if you have switch statements and too many conditional logic; it might be a sign that the behavior is too much hardcoded, and might be refactored in a better way.The main logic and the driver here is that by modifying existing constructs, you’re increasing the likelihood to introduce new bugs and issues to the existing features. If you find yourself refactoring the class and its methods over and over again, you’ll likely end up with some inconsistencies. Also, you’ll sneak in, and update/refactor your existing unit tests to reflect the new reality you created, which is not good practice at all. By striving to design an architecture, where you can add new requirements by just creating new constructs, you’ll get a more robust and error-prone solution. That’s all what OCP means.Once that said, now let’s imagine a scenario and try to offer various solutions to the problem in hand. Imagine you have an enterprise application (e.g. ERP solution), and you offering it to various clients, deployed in their premises. One of the requirements of the solution is to calculate some price for some particular workflow. As the number of your clients increases, they all want some minor, or major changes in how this calculation is done. The rest of the workflow remains the same, but the price calculation varies significantly depending on the customer.Solution 1Let’s start with the most simple solution. We have one single method which holds all the logic and handles all the requirements.public class MyService{ private readonly decimal someBaseValue; public MyService(decimal someBaseValue) { this.someBaseValue = someBaseValue; } public decimal GetCalculatedPrice(string clientType) { if (clientType.Equals(\"clientType1\")) { return Math.Round(this.someBaseValue * 10, 2); } else if (clientType.Equals(\"clientType2\")) { return Math.Round(this.someBaseValue * 10 + 3, 0); } else if (clientType.Equals(\"clientType3\")) { return (this.someBaseValue - 2) * 5; } throw new NotSupportedException(); }}Obviously, this is not the best option. If we get a new client who requires new calculation rules, we’ll end up updating the same method. We’ll be forced to update/modify the unit test for this method as well. Not a great place to be.Solution 2In order to improve this, first and foremost, we can start by extracting the calculation logic for each customer into separate methods.But, let’s take a moment and re-think it. Why methods? Why not separate classes? Why not classes which implement a particular interface?public interface IPriceCalculator{ decimal GetPrice(decimal someBaseValue);}public class Type1Calculator : IPriceCalculator{ public decimal GetPrice(decimal someBaseValue) { return Math.Round(someBaseValue * 10, 2); }}public class Type2Calculator : IPriceCalculator{ public decimal GetPrice(decimal someBaseValue) { return Math.Round(someBaseValue * 10 + 3, 0); }}public class Type3Calculator : IPriceCalculator{ public decimal GetPrice(decimal someBaseValue) { return (someBaseValue - 2) * 5; }}public class MyService{ private readonly decimal someBaseValue; public MyService(decimal someBaseValue) { this.someBaseValue = someBaseValue; } public decimal GetCalculatedPrice(string clientType) { IPriceCalculator calculator = null; if (clientType.Equals(\"clientType1\")) { calculator = new Type1Calculator(); } else if (clientType.Equals(\"clientType2\")) { calculator = new Type2Calculator(); } else if (clientType.Equals(\"clientType3\")) { calculator = new Type3Calculator(); } if (calculator != null) { return calculator.GetPrice(this.someBaseValue); } throw new NotSupportedException(); }}We extracted the logic for each client into a separate class. We defined a contract/interface to standardize the output of the operation. We still have the ugly conditional logic, but this seems much better. If we have to change the calculation for any client, we’ll change the corresponding class, and nothing else.Note: In the newer versions, in C#8 and C#9, we can simplify the conditions with pattern matching. For the sake of simplicity, we’ll stick to the traditional IFs.Solution 3Would be nice to get rid of the “selector” logic completely. What if each calculator holds the information for whom they’re meant to?public interface IPriceCalculator{ string ClientType { get; } decimal GetPrice(decimal someBaseValue);}public class Type1Calculator : IPriceCalculator{ public string ClientType { get; } = \"clientType1\"; public decimal GetPrice(decimal someBaseValue) { return Math.Round(someBaseValue * 10, 2); }}public class Type2Calculator : IPriceCalculator{ public string ClientType { get; } = \"clientType2\"; public decimal GetPrice(decimal someBaseValue) { return Math.Round(someBaseValue * 10 + 3, 0); }}public class Type3Calculator : IPriceCalculator{ public string ClientType { get; } = \"clientType3\"; public decimal GetPrice(decimal someBaseValue) { return (someBaseValue - 2) * 5; }}public class MyService{ private readonly IEnumerable&lt;IPriceCalculator&gt; priceCalculators; public MyService(IEnumerable&lt;IPriceCalculator&gt; priceCalculators) { this.priceCalculators = priceCalculators; } public decimal GetCalculatedPrice(decimal someBaseValue, string clientType) { var calculator = priceCalculators.FirstOrDefault(x =&gt; x.ClientType.Equals(clientType)); if (calculator == null) { throw new NotSupportedException(); } return calculator.GetPrice(someBaseValue); }}Also, you should register all implementations in your DI containerservices.AddScoped&lt;IPriceCalculator, Type1Calculator&gt;();services.AddScoped&lt;IPriceCalculator, Type2Calculator&gt;();services.AddScoped&lt;IPriceCalculator, Type3Calculator&gt;();This is way better, right? We have no hard-coded conditional logic at all. Even if we have to create a new calculator, we can simply add a new class (new implementation) and wire it up to our DI container.The downside of this approach is that we’re creating instances we don’t actually need. We need one single calculator, and yet we’re instantiating all of them.Solution 4In this last option, we’ll try to mitigate the last issue (not an issue per se). We want to inject one single implementation, the required one for the corresponding client. And this is the case where dynamic/runtime DI configuration comes really handy.Configuration in the appsetting.json{ \"Bindings\": { \"binding1\": { \"service\": \"SampleLibrary.IPriceCalculator, SampleLibrary\", \"implementation\": \"SampleLibrary.Type1Calculator, SampleLibrary\", \"scope\": \"scoped\" } }}Wiring it up in DI containerpublic class Startup{ public IConfiguration Configuration { get; } public Startup(IConfiguration configuration) { Configuration = configuration; } public void ConfigureServices(IServiceCollection services) { services.AddBindings(Configuration); }}No more we need the type identifier in the interfacepublic interface IPriceCalculator{ decimal GetPrice(decimal someBaseValue);}And the actual implementation in our servicepublic class MyService{ private readonly IPriceCalculator priceCalculator; public MyService(IPriceCalculator priceCalculator) { if (priceCalculator == null) throw new ArgumentNullException(nameof(priceCalculator)); this.priceCalculators = priceCalculators; } public decimal GetCalculatedPrice(decimal someBaseValue) { return calculator.GetPrice(someBaseValue); }}I do believe this is quite a clean solution. No longer instantiating and injecting unnecessary objects, no longer you have hard-coded selector logic, and no longer you end up modifying the same constructs over and over again. For each client, you create and deploy a different configuration file, injecting the correct and requested implementation.ConclusionHaving the option to switch the implementations easily and modify the behavior of the solution on the fly, is quite a handy and nice feature. The only caveat is that we gave up the commodity of the compile-time checks. On the other hand, it’s not that the configuration issues/errors will just sneak into our code. If there is a misconfiguration we’ll get a runtime exception during the startup, so it’s easy to spot and rectify the issues. Yet, it’s not the same experience as fixing issues in a development environment, and that’s something to consider.It’s important to remember, not necessarily you have to move all DI configurations to an external config file. You still can do the binding for most of the services in your code. Then, for particular cases where you have multiple implementations, and you want more flexibility, you can take only those definitions out.Anyhow this is a love/hate game :) The important point is having a choice, and then you go and choose your own design and whatever fits you most!Cheers!" }, { "title": "How to implement auditing on your entities!", "url": "/posts/how-to-implement-auditing-on-your-entities/", "categories": "Software Development", "tags": "EFCore, Auditing", "date": "2020-11-28 17:00:00 +0100", "snippet": "Not rarely we require to apply some basic auditing information for our entities. Although we may configure full audit features on the Database side, sometimes it’s handy to have this information as...", "content": "Not rarely we require to apply some basic auditing information for our entities. Although we may configure full audit features on the Database side, sometimes it’s handy to have this information as part of your entities.Of course, we want this feature to be processed behind the scenes, automatically, and not deal with it manually. If you’re using Entity Framework (EF6, EFCore), implementation is quite straightforward.Auditable EntitiesLet’s create a base class that will hold the audit information. Once created, use it as a base class for all entities that you want to apply auditing.public class AuditableEntity{ public DateTime? AuditCreatedTime { get; set; } public string AuditCreatedByUserId { get; set; } public string AuditCreatedByUsername { get; set; } public DateTime? AuditModifiedTime { get; set; } public string AuditModifiedByUserId { get; set; } public string AuditModifiedByUsername { get; set; }}I like to prefix all the properties (and DB columns) with “Audit”. It helps me easily distinct these values, and I like having them grouped when shown by IntelliSense. You can have these properties named differently, and still, have specific names configured for the DB columns.Other than that, on top of UserID I usually tend to persist the Username of the user too. This way, if you want to utilize this information and display it on the UI, you won’t have to query the Identity persistence each time.User information providerIf you’re using ASP.NET Core, you can access the authenticated user’s information through HttpContextAccessor. But, you may want to keep your persistence infrastructure in a separate project, in which case you won’t have direct access to this property.Let’s create a simple class that will encapsulate the current user’s information, and provide it wherever is required.public class CurrentUserProvider : ICurrentUserProvider{ private readonly IHttpContextAccessor _httpContextAccessor; public CurrentUserProvider(IHttpContextAccessor httpContextAccessor) { _httpContextAccessor = httpContextAccessor; } public string UserId =&gt; _httpContextAccessor.HttpContext?.User?.FindFirstValue(ClaimTypes.NameIdentifier); public string Username =&gt; _httpContextAccessor.HttpContext?.User?.FindFirstValue(ClaimTypes.Name);}Also, you should register these services in your DI container as followingservices.AddSingleton&lt;IHttpContextAccessor, HttpContextAccessor&gt;();services.AddScoped&lt;ICurrentUserProvider, CurrentUserProvider&gt;();Implement auditingNow that we have the supporting infrastructure ready, let’s make a few modifications in the DbContext class.First, modify the constructor and inject the ICurrentUserProvider implementation.private readonly ICurrentUserProvider currentUserProvider;public MyDbContext(DbContextOptions&lt;MyDbContext&gt; options, ICurrentUserProvider currentUserProvider) : base(options){ this.currentUserProvider = currentUserProvider;}Then, override the SaveChangesAsync method and add the actual implementation for the auditing.public async override Task&lt;int&gt; SaveChangesAsync(CancellationToken cancellationToken = default){ var addedEntries = ChangeTracker.Entries&lt;AuditableEntity&gt;().Where(x =&gt; x.IsAdded()); var modifiedEntries = ChangeTracker.Entries&lt;AuditableEntity&gt;().Where(x =&gt; x.IsModified()); foreach (var entry in addedEntries) { entry.CurrentValues[nameof(AuditableEntity.AuditCreatedTime)] = DateTime.Now; entry.CurrentValues[nameof(AuditableEntity.AuditCreatedByUserId)] = currentUserProvider?.UserId; entry.CurrentValues[nameof(AuditableEntity.AuditCreatedByUsername)] = currentUserProvider?.Username; } foreach (var entry in modifiedEntries) { entry.CurrentValues[nameof(AuditableEntity.AuditModifiedTime)] = DateTime.Now; entry.CurrentValues[nameof(AuditableEntity.AuditModifiedByUserId)] = currentUserProvider?.UserId; entry.CurrentValues[nameof(AuditableEntity.AuditModifiedByUsername)] = currentUserProvider?.Username; } return await base.SaveChangesAsync(cancellationToken);}You may notice, I’m using IsAdded and IsModified extensions, instead of directly utilizing the EntityState enum. This is important if your entities hold owned types (e.g. value objects). You want to consider the whole entity as modified if the owned types are added/modified.public static class ChangeTrackerExtensions{ public static bool IsAdded(this EntityEntry entry) =&gt; entry.State == EntityState.Added; public static bool IsModified(this EntityEntry entry) =&gt; entry.State != EntityState.Added &amp;&amp; (entry.State == EntityState.Modified || entry.References.Any(r =&gt; r.TargetEntry != null &amp;&amp; r.TargetEntry.Metadata.IsOwned() &amp;&amp; (r.TargetEntry.State == EntityState.Added || r.TargetEntry.State == EntityState.Modified)));}And that’s all. On each save, the auditing information will be persisted for your chosen entities." }, { "title": "What is a domain model?", "url": "/posts/what-is-a-domain-model/", "categories": "Blogging, Software Development", "tags": "ddd, design patterns, software architecture", "date": "2020-11-05 12:00:00 +0100", "snippet": "I’ve been asked this question very often, especially by the new developers. So, let’s try to explain the term and clear some misconceptions.The term is being popularized with the adoption of DDD (d...", "content": "I’ve been asked this question very often, especially by the new developers. So, let’s try to explain the term and clear some misconceptions.The term is being popularized with the adoption of DDD (domain-driven design), but nowadays I think is broadly misunderstood. Usually, I get the following explanations: It’s just the data objects that map to your DB tables. Actually, the entities. If you’re doing code-first, the entities are your domain model. You must have behavior in your data objects. You shouldn’t have anemic objects.Almost all of the answers I get, usually have to do with some “data constructs”. Well, none of that is correct, or at least it’s just partially true.I think, unfortunately, we have overused the term “model”; to a point where it got highly ambiguous meaning. Being used in so many different contexts, developers attach their own meaning to any term containing the word “model”.In the context of domain model, it simply means “modeling”. See, as the architects for an instance, try to model the layout of the city in a map through drawing; we’re trying to model the business rules and processes through code. We try capture each procedure, each rule, each workflow of the particular business; and convert all of that into code. We do that with all the available tools and constructs we have in a certain programming language.So, domain model is a collection of constructs that accurately models one particular business domain. Simply put, it’s a collection of entities, enumerations, value objects, exceptions/custom exceptions, interfaces, services, etc. All of that represents the domain model. Anything that has any impact in describing the business domain. On the other hand, any construct that doesn’t take part in this description; the UI code, infrastructure related code, persistence objects, various tools, etc; are considered as third-party/outer concerns in the context of DDD.The domain model should change only when business requirements change, and never otherwise. Whatever construct fits into this category, is part of your domain model." }, { "title": "How to build a blog with GitHub Pages and Jekyll!", "url": "/posts/how-to-build-blog-ghpages-jekyll/", "categories": "Blogging, Tutorial", "tags": "Productivity", "date": "2020-11-04 12:00:00 +0100", "snippet": "I recently decided to change my blogging platform. I simply want to focus on writing new content, and not deal with the peculiarities of different CMS tools. Ideally, I wanted the whole content to ...", "content": "I recently decided to change my blogging platform. I simply want to focus on writing new content, and not deal with the peculiarities of different CMS tools. Ideally, I wanted the whole content to be stored in a git repository, and to be able to post articles in a form of markdown files. The design might not be anything fancy, but should be clean and easy to navigate for the users.First of all, I defined my requirements and listed the features I want the site to have. Use git repository as storage, and publish it publicly on GitHub. Write articles in form of .md files, and publish them by simply pushing to the repository. I should be able to divide the articles into categories and should be able to assign tags. It should support pagination. It should support searching through articles (title and description) It should support comments (disquss maybe?) It should list related articles, recent articles, etc. Users should be able to browse through categories or tags Users should be able to browse the archive.There are plenty of options that would fit these requirements. But, ultimately I decided to host my blog as a static site on GitHub Pages. It has built-in support for Jekyll (as a static site generator), it’s integrated with GitHub repositories (obviously) and on top of that I can use GitHub actions if there is a need for complex builds/deployments.In the following sections, I’ll walk you through the whole process and provide more details for each step.Preparing the custom domainIf you want to use your custom domain name for GH Pages, then you need to update your DNS records, so they correctly point to GH hosting servers. I had my domains on GoDaddy, but since Cloudflare offers free SSL certificates (even on the free tier), I moved the DNS management to Cloudflare. If you don’t have SSL for your domain, then I strongly urge to get one.Create an account and create your domains on Cloudlfare. Once you do that they will provide their nameservers, which you will have to provide to your registrar. Simply login to your domain registrar’s site and change the nameservers into the ones provided by Cloudflare. The change propagation might take some time (up to 48 hours in some cases).Once the change is validated by Cloudflare, then update your DNS records as shown belowThe A records should point to the GH servers, and you can copy these lines as they are. You should also add a CNAME for www subdomain. You can ignore the MX record, it has to do with mail configuration (I’m pointing to my mail server). Lastly, if you add your site to Google Search, you will need a TXT record for site verification.This is all you need in order to use your custom domain.Choosing a templateThere are a lot of jekyll templates that you can choose from. Here is a list of some theme galleries jamstackthemes.dev jekyllthemes.io jekyll-themes.comI want a simple and clean template. No longer I want any additional side content (e.g twitter feeds).You can find the template I chose in the following link. The author, Cotes Chung, did an awesome job. Also, I rarely see such an organized and comprehensive GH repository. Documentation, examples, contribution guidelines, templates for issues. Kudos to the author!I did additional changes to the template, and you can find my repo here. If you like it, please feel free to use it as you wish. Still, I urge you to give some credit to the original author (starring the project at least).InstallationI will try to simplify the installation steps here. If you need more information, check here. If you need to build the site locally, then you will need a linux environment. Now that we have WSL2 on Windows that’s quite easy to accomplish, and it’s not worth trying to make it work natively on Windows. Fork from Cotes Chung’s repository or Fati Iseni’s Blog (based on your preference). You can copy the content instead of forking if that’s more convenient for you. Change the name of the repository into {username}.github.io, where username is your github username. You get one site per GitHub account and organization, and unlimited project sites. Clone the repository locally git clone https://github.com/USERNAME/USERNAME.github.io.git (Now you will need to prepare the local environment) Install Ruby, RubyGems, and Jekyl. Additional information here sudo apt-get install ruby-full build-essential zlib1g-dev echo '# Install Ruby Gems to ~/gems' &gt;&gt; ~/.bashrc echo 'export GEM_HOME=\"$HOME/gems\"' &gt;&gt; ~/.bashrc echo 'export PATH=\"$HOME/gems/bin:$PATH\"' &gt;&gt; ~/.bashrc source ~/.bashrc gem install jekyll bundler Go to the root directory of the project and complete the installation of the Jekyll plugins bundle install Install a few more tools that we will need. sudo apt-get install coreutils sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys CC86BB64 sudo add-apt-repository ppa:rmescandon/yq sudo apt update sudo apt install yq -y Now that the required tools are installed, go to the root directory of the project and run the init job. You need to do this only once, it cleans the repository and makes it ready for you. bash tools/init.sh Update the configuration and customize it for your needs Update the _config.yml file with your information Update the CNAME file, enter your domain name Update the images under /assets/img/ Run the site locally bash tools/run.sh Deploying on GitHub PagesUsually, pushing the content is all you need. GitHub Pages will build it automatically and your site will be up and running.In our case, we’re using build scripts in order to generate some additional content (list of tags, categories, archives, etc). GitHub Pages does not allow script execution (for safety reasons), so we will use GitHub Actions to build the site manually using our own build script.Everything is already configured (there is yaml file under .github folder), you just have to follow these steps Push the content to master branch. Once the build is complete, new branch will be generated gh-pages Open the repository setting under GitHub, and change the publish source to gh-pagesNow, your site will be up and running.Publishing contentIn order to publish new articles, all you have to do is create a new .md file under posts folder. Each file should contain the following information at the top---title: \"Welcome to my new blog!\"date: 2020-10-30 12:00:00 +0100description: My new blog based on GitHub Pages and Jekyll.categories: [Blogging, Tutorial]tags: [Productivity]image: /assets/img/pozitron-cover.pngpin: true# math: true# toc: true---If you want to learn more about all the available variables that you can use, then click here." }, { "title": "Welcome to my new blog!", "url": "/posts/welcome-to-my-new-blog/", "categories": "Blogging, Tutorial", "tags": "Productivity", "date": "2020-10-30 12:00:00 +0100", "snippet": "This is my new blog, set up with GitHub Pages and Jekyll.This was on my plan for a long time, and finally I found some time to research and fix this. I want simple and basic platform, where I can f...", "content": "This is my new blog, set up with GitHub Pages and Jekyll.This was on my plan for a long time, and finally I found some time to research and fix this. I want simple and basic platform, where I can focus all my energy into writing content, and not dealing with various technical/design details. The other platforms (e.g. Wordpress) served me well for years (or not?), but, I think it’s time for changes and time to simplify the matter even further.Objectives and expectationsWhile I was trying to find an alternative platform for blogging, first and foremost I clearly defined my objectives. It should be simple and clean It should contain only articles. I do not want any side content anymore. No tweet feeds, no any additional content. I’m already familiar with markdown syntax. I want to write the articles in the form of .md files The content should reside on a git repository. GitHub is my preferred choice. The repository not necessarily should be private, even better, let it be a public one. I want to write an article, save it as a markdown file, and push it to a git repository. Prefer to host the repository on GitHub. It will act as a CMS, and I can write content directly from the web.After a quick research, I found that there are plenty of options that fit these requirements. But, I found that GitHub Pages is quite appealing platform. It has built-in support for Jekyll, which is a static web site generator. GH Pages is already integrated with GitHub repositories (obviously), and on top of that there is an option to set-up a custom domain for your blog (the default assigned url is yourusername.github.io).All I had to do is find a simple/clean template, and spend couple of hours to finish this setup. And here it is!In the next article, I’ll try to provide step-by-step tutorial on how to setup your own blog in the same way.Cheers!" } ]
